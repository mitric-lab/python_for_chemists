<!DOCTYPE HTML>
<html lang="de" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Numerical Optimisation - Programmierkurs für Chemiker</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">
        <link rel="stylesheet" href=".././theme/catppuccin-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish-custom.css">
        <link rel="stylesheet" href=".././theme/icomoon.css">
        <link rel="stylesheet" href=".././theme/pagetoc.css">
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "mocha" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Inhaltsverzeichnis</li><li class="chapter-item expanded "><a href="../00-preface.html"><strong aria-hidden="true">0.</strong> Vorwort</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../00-preface/01-motivation.html"><strong aria-hidden="true">0.1.</strong> Motivation</a></li><li class="chapter-item "><a href="../00-preface/02-getting_started.html"><strong aria-hidden="true">0.2.</strong> Erste Schritte</a></li><li class="chapter-item "><a href="../00-preface/03-mdbook_usage.html"><strong aria-hidden="true">0.3.</strong> Bedienung dieser Website</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../01-regression.html"><strong aria-hidden="true">1.</strong> Regression Analysis</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../01-regression/01-least_squares.html"><strong aria-hidden="true">1.1.</strong> Least Squares</a></li><li class="chapter-item "><a href="../01-regression/02-linear_regression.html"><strong aria-hidden="true">1.2.</strong> Linear Regression</a></li><li class="chapter-item expanded "><a href="../01-regression/03-numerical_optimisation.html" class="active"><strong aria-hidden="true">1.3.</strong> Numerical Optimisation</a></li><li class="chapter-item "><a href="../01-regression/04-nonlinear_regression.html"><strong aria-hidden="true">1.4.</strong> Nonlinear Regression</a></li></ol></li><li class="chapter-item expanded "><a href="../02-differential_equations.html"><strong aria-hidden="true">2.</strong> Differential Equations</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../02-differential_equations/01-initial_value_problem.html"><strong aria-hidden="true">2.1.</strong> Initial Value Problem</a></li><li class="chapter-item "><a href="../02-differential_equations/02-euler_method.html"><strong aria-hidden="true">2.2.</strong> Euler Method</a></li><li class="chapter-item "><a href="../02-differential_equations/03-runge_kutta.html"><strong aria-hidden="true">2.3.</strong> Runge-Kutta Method</a></li><li class="chapter-item "><a href="../02-differential_equations/04-finite_differences.html"><strong aria-hidden="true">2.4.</strong> Finite-Difference Method</a></li></ol></li><li class="chapter-item expanded "><a href="../04-evd_and_svd.html"><strong aria-hidden="true">3.</strong> Eigenvalue and Singular Value Decomposition</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../04-evd_and_svd/01-eigenvalue_decomposition.html"><strong aria-hidden="true">3.1.</strong> Eigenvalue Decomposition</a></li><li class="chapter-item "><a href="../04-evd_and_svd/02-singular_value_decomposition.html"><strong aria-hidden="true">3.2.</strong> Singular Value Decomposition</a></li><li class="chapter-item "><div><strong aria-hidden="true">3.3.</strong> Principal Component Analysis</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.4.</strong> Principal Coordinates Analysis</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.5.</strong> Linear Equation Systems</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Maschinelles Lernen</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">4.1.</strong> Überwachtes Lernen</div></li><li class="chapter-item "><div><strong aria-hidden="true">4.2.</strong> Unüberwachtes Lernen</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Neuronale Netzwerke</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">5.1.</strong> Single-Layer-Perzeptron</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.2.</strong> Multi-Layer-Perzeptron</div></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded affix "><div>Zusammenfassung und Ausblick</div></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded affix "><a href="../psets/01.html">Problem Set 1</a></li><li class="chapter-item expanded affix "><a href="../psets/02.html">Problem Set 2</a></li><li class="chapter-item expanded affix "><div>Problem Set 3</div></li><li class="chapter-item expanded affix "><div>Problem Set 4</div></li><li class="chapter-item expanded affix "><div>Problem Set 5</div></li><li class="chapter-item expanded affix "><div>Problem Set 6</div></li><li class="chapter-item expanded affix "><div>Sample Exam</div></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Programmierkurs für Chemiker</h1>

                    <div class="right-buttons">
			<!--print button-->
			<a href="../print.html" title="Buch drucken" aria-label="Buch drucken">
			    <i id="print-button" class="fa fa-print"></i>
			</a>

                        
                        <!-- WueCampus button -->
                        <script>
                            var wueCampusLink = "https://wuecampus.uni-wuerzburg.de/moodle/course/view.php?id=73209";
                        </script>
			<a href="javascript:void(0);" onclick="window.open(wueCampusLink)" target="_blank" rel="noopener noreferrer" title="WueCampus-Kursraum besuchen" aria-label="WueCampus-Kursraum besuchen">
			    <i id="wuecampus-button" class="icon-uw"></i>
			</a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h2 id="numerical-optimisation"><a class="header" href="#numerical-optimisation">Numerical Optimisation</a></h2>
<p>In many scientific problems, we are interested in finding the global minimum or maximum of a function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, or the parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> that yield this minimum or maximum. For example, in the previous chapter, we wanted to find the parameters that minimise the loss function, and in many applications in quantum chemistry, we want to find the coefficients that minimise the energy of a molecule. Since maximising a function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> is equivalent to minimising <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, we will only discuss minimisation in the following. Because analytical solutions do not exist for most of these problems, we need to find ways to determine the global minimum or maximum of a function numerically.</p>
<p>In the vast majority of cases, finding the global minimum is impossible because the given function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> is highly non-convex (i.e., it has many local minima) or is too high-dimensional (i.e., it has many parameters). However, there are several methods that allow us to find a <strong>local minimum</strong> of a function, such as the gradient descent method.</p>
<h3 id="theoretical-foundations"><a class="header" href="#theoretical-foundations">Theoretical Foundations</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is an <em>iterative method</em> that starts from a given initial set of parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span> and follows the direction of steepest descent of the function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> at each step. Mathematically, one step of the algorithm is formulated as
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0935em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0935em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:gradient_descent"></span></span><span class="tag"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.10</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> represents the estimate of the minimum at step <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>. The superscript <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> is used to denote the iteration step and should not be confused with exponentiation. The gradient of the <em>objective function</em> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> with respect to the parameters at step <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> is denoted as <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8543em;vertical-align:-0.65em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1339em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0528em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.927em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1339em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0528em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.927em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.2043em;"><span style="top:-3.6029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord amsrm mtight">⊺</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>The proportionality constant <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is called the <em>step size</em> or <em>learning rate</em>. The procedure is repeated until one or more stopping conditions are met. Typical stopping conditions for iterative optimisation methods are:</p>
<ul>
<li>The change in the function value is smaller than a threshold</li>
<li>The change in the parameters is smaller than a threshold</li>
<li>A maximum number of iterations is reached</li>
<li>The norm of the gradient is smaller than a threshold</li>
</ul>
<p>The step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is a crucial parameter of gradient descent, as it influences both the convergence speed and stability of the method. A value that is too small can lead to very slow convergence, while a value that is too large can cause the method to diverge.</p>
<p>To use gradient descent, we need access to the gradient of the objective function with respect to the parameters. If this is not available analytically, we must use a numerical approximation. A simple approach is the method of <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>. Here, the tangent of the partial derivative is approximated by the secant, leading to an approximation of the form
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3692em;vertical-align:-0.9978em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.2791em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em;"><span style="top:-2.4231em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9978em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2121em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">h</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:finite_difference_symmetric"></span></span><span class="tag"><span class="strut" style="height:2.5239em;vertical-align:-0.9978em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.11</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th unit vector and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> is a small value that determines the step size of the approximation. More precisely, this equation <a href="#eq:finite_difference_symmetric">(1.11)</a> represents the <em>central finite difference of order 2</em>. There are also <em>one-sided</em> approximations and higher-order approximations, which we will not discuss further here.</p>
<h3 id="implementation"><a class="header" href="#implementation">Implementation</a></h3>
<h4 id="finite-differences"><a class="header" href="#finite-differences">Finite Differences</a></h4>
<p>The first step is to implement the finite difference method. Since we will need to calculate derivatives multiple times, it is useful to implement a <em>function</em>. Functions in the programming context are similar to mathematical functions that transform an input into an output, given a set of rules. In contrast to the code blocks that we have seen so far, the code that defines a function is not executed immediately. Instead, it is only executed when the function is called.</p>
<p>We first import <code>numpy</code>:</p>
<pre><code class="language-python">import numpy as np
</code></pre>
<p>Then we define the function <code>finite_difference</code>, which calculates the gradient of a function <code>func</code> at the point <code>beta</code>:</p>
<pre><code class="language-python">def finite_difference(func, beta, h=1e-5, args=()):
    n = len(beta)
    grad = np.zeros(n)

    for i in range(0, n):
        e = np.zeros(n)
        e[i] = 1
        grad[i] = (
            func(beta + h * e, *args) - func(beta - h * e, *args)
        ) / (2 * h)

    return grad
</code></pre>
<p>The first line of the function is the <em>signature</em>, which defines the name of the function and the names and types of the arguments.</p>
<p>The first line of the function is the <em>signature</em>, which defines the name of the function and its arguments. The function takes four arguments:</p>
<ul>
<li><code>func</code>: The function to be differentiated</li>
<li><code>beta</code>: The point at which to calculate the gradient</li>
<li><code>h</code>: The step size for the finite difference approximation. This is a default argument with value 1e-5, meaning it can be omitted when calling the function</li>
<li><code>args</code>: Additional arguments to pass to <code>func</code>. This is a default argument with value <code>()</code>, an empty tuple. Like <code>h</code>, it can be omitted when calling the function. Default arguments must always come after required arguments in the function signature</li>
</ul>
<div id="admonition-dynamic-typing-in-python" class="admonition admonish-note">
<div class="admonition-title">
<p>Dynamic typing in Python</p>
<p><a class="admonition-anchor-link" href="#admonition-dynamic-typing-in-python"></a></p>
</div>
<div>
<p>In strongly typed languages, function signatures also define the types of the arguments and the return type. Since Python is a dynamically typed language, the types of the arguments are only defined when the function is called. We therefore have to be careful in naming the arguments, such that the type of the argument is clear from the name.</p>
</div>
</div>
<p>In the body of the function, we first determine the dimension of the parameters <code>beta</code> and store it in the integer <code>n</code>. The gradient of a real-valued function with <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> variables is a vector of length <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>. Therefore, we initialize the variable <code>grad</code> as a numpy array of length <code>n</code> filled with zeros using the function <a href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html"><code>np.zeros</code></a>.</p>
<p>We then use a <a href="https://docs.python.org/3/tutorial/controlflow.html#for-statements"><em>for</em>-loop</a> to calculate the gradient component by component. The loop iterates over all dimensions <code>i</code> from <code>0</code> to <code>n - 1</code>. Note that Python indices start at <code>0</code> and the built-in function <a href="https://docs.python.org/3/library/functions.html#func-range"><code>range</code></a> excludes the end value <code>n</code>.</p>
<p>In each iteration of the loop, we first define the unit vector <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> by first creating a zero array of length <code>n</code> with <code>np.zeros(n)</code> and then setting the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th component to <code>1</code>. We then can calculate the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th entry of the gradient array <code>grad</code> according to Eq. <a href="#eq:finite_difference_symmetric">(1.11)</a>. Additionally, we pass the arguments <code>args</code> to the function <code>func</code>, with a star <code>*</code> in front of the variable. The use of <code>*</code> acts as a unary operator at this point, which unpacks an object into its components (see <a href="https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists"><em>unpacking</em></a>). This means that the function <code>func</code> accepts the further arguments in the tuple <code>args</code> <strong>individually</strong> after the first argument. We will see an example for this in the next section.</p>
<p>After the loop completes, we return the gradient array <code>grad</code>.</p>
<h4 id="objective-function"><a class="header" href="#objective-function">Objective Function</a></h4>
<p>Next, we implement the function whose value we want to minimise. This can be any function, but we choose the least squares loss function <a href="02-linear_regression.html#eq:least_squares_loss_linear">(1.8)</a> of the linear regression model, which we implement as follows:</p>
<pre><code class="language-python">def least_squares_loss(beta, x, y):
    return np.sum((y - (beta[0] + beta[1] * x))**2)
</code></pre>
<p>Inline with our previous notation, we use the variable <code>beta</code> to represent the parameters, which we will later optimise. The second and third arguments are <code>x</code> and <code>y</code>, which are the data points.</p>
<p>The gradient of the least squares loss function with respect to the parameters can now easily be calculated using the function <code>finite_difference</code> that we implemented earlier. But first, let’s implement the gradient descent algorithm.</p>
<h4 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient Descent</a></h4>
<p>According to Eq. <a href="#eq:gradient_descent">(1.10)</a>, the algorithm requires the gradient of the objective function, the initial parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span>, the step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>, and the maximum number of iterations. Instead of the gradient, we can also pass the function directly to the algorithm and estimate the gradient numerically using the <code>finite_difference</code> function. As a stopping condition, we use a combination of the maximum number of iterations and the norm of the gradient. Therefore, we need the additional arguments <code>max_iter</code> and <code>max_norm</code>. We again need the argument <code>args</code>, which contains the additional arguments for the function that we want to optimise.</p>
<pre><code class="language-python">def gradient_descent(func, beta0, alpha=0.001, 
                     max_norm=1e-6, max_iter=10000, args=()):
    beta = np.copy(beta0)
    for niter in range(0, max_iter):
        grad = finite_difference(func, beta, args=args)
        beta = beta - alpha * grad
        if np.linalg.norm(grad) &lt; max_norm:
            break
    if niter == max_iter - 1:
        print('Warning: Maximum iterations reached. '
              'Result may not be reliable.')
    
    return beta, niter
</code></pre>
<p>In the body of the function, we first explicitly copy the array <code>beta0</code> using <code>np.copy</code> and store the copy in the variable <code>beta</code>. This is necessary because Python does not copy arrays by default, but only stores references to them. Illustrating this with an example, if we simply rename <code>a</code> to <code>b</code>, a change in <code>a</code> will also cause a change in <code>b</code>, and <em>vice versa</em>.</p>
<p>Then we use a <code>for</code>-loop that iterates the variable <code>niter</code> from <code>0</code> to <code>max_iter - 1</code>. In each iteration, we calculate the gradient <code>grad</code> by calling the function <code>finite_difference</code> with the arguments <code>func</code>, <code>beta</code> and <code>args</code>. Then we use Eq. <a href="#eq:gradient_descent">(1.10)</a> to update the variable <code>beta</code>. Note that Python evaluates the right-hand side of the assignment before assigning the result to <code>beta</code>. After that, we calculate the norm of the gradient with the function <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code>np.linalg.norm</code></a> and check if it is smaller than <code>max_norm</code>. If so, we break the loop with the <code>break</code> command.</p>
<p>After the last iteration, we check if the variable <code>niter</code> has reached the maximum number of iterations <code>max_iter</code>. If so, it means that the gradient is still large, and the method may not have converged, and we issue a warning.</p>
<p>At the end, we return the optimum <code>beta</code> and the number of iterations <code>niter</code> that were actually needed.</p>
<h3 id="application"><a class="header" href="#application">Application</a></h3>
<p>Let’s apply our implementation to the data from Chapter <a href="02-linear_regression.html">1.2</a>. First, we define our data arrays:</p>
<pre><code class="language-python">concentrations = [
    2.125, 4.250, 6.375, 8.500, 10.63, 12.75, 14.88, 17.00, 19.13, 21.25,
    23.38, 25.50, 27.63, 29.75, 31.88, 34.00, 36.13, 38.25, 40.38, 42.50,
]
absorbances = [
    0.0572, 0.1391, 0.2049, 0.2754, 0.3420, 
    0.4139, 0.4956, 0.5815, 0.6806, 0.7481,
    0.8242, 0.9130, 1.0043, 1.0809, 1.1511,
    1.2483, 1.3373, 1.4027, 1.4927, 1.5853,
]
concentrations = np.array(concentrations)
absorbances = np.array(absorbances)
</code></pre>
<p>Then we set an initial guess for the parameters and run gradient descent:</p>
<pre><code class="language-python">beta_guess = np.array([1.0, 1.0])
beta_opt, niter = gradient_descent(
    least_squares_loss, 
    beta_guess,
    alpha=0.00005,
    max_iter=100000,
    args=(concentrations, absorbances),
)

assert np.isclose(beta_opt[0], -0.04907034)
assert np.isclose(beta_opt[1], 0.03800109)
</code></pre>
<p>The optimal parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> are identical to those of the analytical solution. On the author’s computer, 34,683 iterations were needed to satisfy the stopping condition. The exact number of iterations may vary slightly depending on the hardware. If you choose the step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> slightly larger, fewer iterations are needed. However, if <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is too large, the method diverges.</p>
<div id="admonition-task" class="admonition admonish-tip">
<div class="admonition-title">
<p>Task</p>
<p><a class="admonition-anchor-link" href="#admonition-task"></a></p>
</div>
<div>
<p>Try changing the step size <code>alpha</code> and observe how the number of iterations changes.</p>
</div>
</div>
<h3 id="built-in-optimisation"><a class="header" href="#built-in-optimisation">Built-in Optimisation</a></h3>
<p>Since optimisation is a very general problem, there are many implementations of various algorithms in libraries such as <a href="https://docs.scipy.org/doc/scipy/reference/optimize.html"><code>scipy.optimize</code></a>. We want to use the function <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code>scipy.optimize.minimize</code></a> to find the optimal parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> and compare them with our implementation:</p>
<pre><code class="language-python">from scipy.optimize import minimize

res = minimize(
    least_squares_loss,
    beta_guess,
    args=(concentrations, absorbances),
    method='CG',
    jac=lambda beta, x, y: finite_difference(least_squares_loss, beta, args=(x, y)),
    options={'maxiter': 10000, 'gtol': 1e-6},
)

beta_opt = res.x
niter = res.nit
assert np.isclose(beta_opt[0], -0.04907034)
assert np.isclose(beta_opt[1], 0.03800109)
</code></pre>
<p>When calling the function <code>minimize</code>, we only need to specify the objective function <code>objective_function</code> and the initial point <code>beta_guess</code>. The numerical gradient calculation is handled internally.</p>
<div id="admonition-gradient-calculation" class="admonition admonish-note">
<div class="admonition-title">
<p>Gradient Calculation</p>
<p><a class="admonition-anchor-link" href="#admonition-gradient-calculation"></a></p>
</div>
<div>
<p>If the analytical gradient is available, or you have a good estimate of
the gradient at hand, you can pass it as the <code>jac</code> argument to the
<code>minimize</code> function, which can speed up the optimisation process.
The <code>jac</code> argument expects a function of the form <code>jac(beta, arg1, arg2, ...)</code>,
which takes the current parameters <code>beta</code> and the additional arguments
and returns the gradient. Therefore, we have to use a <code>lambda</code> function
to convert the <code>finite_difference</code> function into the required format.</p>
<p>If the <code>jac</code> argument is not provided, the <code>minimize</code> function will
automatically calculate the gradient using the finite difference method.
Therefore, it is totally fine to not provide the <code>jac</code> argument
in this case.</p>
</div>
</div>
<p>With the argument <code>method='CG'</code>, we have selected the
<a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method"><i>nonlinear <strong>C</strong>onjugate <strong>G</strong>radient method</i></a>. This method is an improvement of the gradient method and reaches the minimum in <strong>only 2 iterations</strong> on the author’s computer.</p>
<details id="admonition-other-optimisation-methods" class="admonition admonish-note">
<summary class="admonition-title">
<p>Other Optimisation Methods</p>
<p><a class="admonition-anchor-link" href="#admonition-other-optimisation-methods"></a></p>
</summary>
<div>
<p>In <code>minimize</code>, there are several minimisation methods available. An overview can be found in the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">documentation</a> of this function. Two important methods are:</p>
<ul>
<li><code>method='Nelder-Mead'</code>:
The <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead method</a> is a heuristic method that does not require gradient calculation. It is particularly useful when gradient calculation is computationally expensive or when the gradient is noisy. This method is especially suitable for regressions with experimental data.</li>
<li><code>method='BFGS'</code>:
The <a href="https://en.wikipedia.org/wiki/BFGS_method"><strong>B</strong>royden-<strong>F</strong>letcher-<strong>G</strong>oldfarb-<strong>S</strong>hanno method</a> uses the gradient to approximate the Hessian matrix of the objective function. The Hessian contains the second derivatives of the function with respect to its parameters. This method shows very fast convergence near the minimum and typically requires fewer iterations than other optimisation methods, making it a popular choice.</li>
</ul>
</div>
</details>
<p>As you will see in the exercise, linear regression with the least squares method has a closed-form solution that directly calculates the optimal parameters. So why should we use numerical optimisation? In the context of regression, numerical optimisation allows us to:</p>
<ul>
<li>Use more complicated models that do not have analytical solutions</li>
<li>Implement more sophisticated loss functions, such as the least absolute deviations (see Eq. <a href="01-least_squares.html#eq:least_absolute_deviations_opt">(1.6)</a>)</li>
<li>Introduce additional control over the parameters through <strong>regularisation</strong>, which can improve the model’s general performance</li>
</ul>
<div id="admonition-function-scipyoptimizecurve_fit" class="admonition admonish-note">
<div class="admonition-title">
<p>Function <code>scipy.optimize.curve_fit</code></p>
<p><a class="admonition-anchor-link" href="#admonition-function-scipyoptimizecurve_fit"></a></p>
</div>
<div>
<p>There is also the function <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code>scipy.optimize.curve_fit</code></a>, which performs nonlinear regression directly. However, it is less flexible than the general <code>minimize</code> function, as it only offers a few optimisation methods and uses the least squares loss function. It is suitable for simple regressions and typically requires less code.</p>
</div>
</div>
<hr />
<p><strong>Self-Study Questions</strong></p>
<ol>
<li>
<p>Explain why the step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> in gradient descent is crucial for the algorithm’s performance. What happens if <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is too small or too large?</p>
</li>
<li>
<p>Illustrate graphically why gradient descent converges to a local minimum, and not the global minimum.</p>
</li>
<li>
<p>Compare and contrast the finite difference method for calculating gradients with analytical gradient calculation. What are the advantages and disadvantages of each approach?</p>
</li>
<li>
<p>To understand why we need the <code>lambda</code> function, try to pass the <code>finite_difference</code> function to the <code>jac</code> argument of the <code>minimize</code> function directly. Consult the documentation of <code>scipy.optimize.minimize</code> for detailed information.</p>
</li>
</ol>
<p><strong>Challenge Questions</strong></p>
<ol>
<li>
<p>Show mathematically that one step of the gradient descent method yields a better estimate of the minimum than the initial guess by expanding <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> in a Taylor series around <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> up to first order, using Eq. <a href="#eq:gradient_descent">(1.10)</a>.</p>
</li>
<li>
<p>Think about how to mitigate the problem of gradient descent converging to a local minimum. How could stochasticity help?</p>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../01-regression/02-linear_regression.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../01-regression/04-nonlinear_regression.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../01-regression/02-linear_regression.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../01-regression/04-nonlinear_regression.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src=".././theme/pagetoc.js"></script>


    </div>
    </body>
</html>
