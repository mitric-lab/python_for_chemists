<!DOCTYPE HTML>
<html lang="de" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Numerical Optimisation - Programmierkurs für Chemiker</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">
        <link rel="stylesheet" href=".././theme/catppuccin-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish-custom.css">
        <link rel="stylesheet" href=".././theme/icomoon.css">
        <link rel="stylesheet" href=".././theme/pagetoc.css">
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "mocha" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Inhaltsverzeichnis</li><li class="chapter-item expanded "><a href="00-preface.html"><strong aria-hidden="true">0.</strong> Vorwort</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="00-preface/01-motivation.html"><strong aria-hidden="true">0.1.</strong> Motivation</a></li><li class="chapter-item "><a href="00-preface/02-getting_started.html"><strong aria-hidden="true">0.2.</strong> Erste Schritte</a></li><li class="chapter-item "><a href="00-preface/03-mdbook_usage.html"><strong aria-hidden="true">0.3.</strong> Bedienung dieser Website</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="01-regression.html"><strong aria-hidden="true">1.</strong> Regression Analysis</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="01-regression/01-least_squares.html"><strong aria-hidden="true">1.1.</strong> Least Squares</a></li><li class="chapter-item "><a href="01-regression/02-linear_regression.html"><strong aria-hidden="true">1.2.</strong> Linear Regression</a></li><li class="chapter-item "><a href="01-regression/03-numerical_optimisation.html"><strong aria-hidden="true">1.3.</strong> Numerical Optimisation</a></li><li class="chapter-item "><div><strong aria-hidden="true">1.4.</strong> Nonlinear Regression</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> Differentialgleichungen</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">2.1.</strong> Anfangswertproblem</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.2.</strong> Euler-Verfahren</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.3.</strong> Runge-Kutta-Verfahren</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.4.</strong> Finite-Differenzen-Verfahren</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> Eigenwert- und Singulärwertzerlegung</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">3.1.</strong> Eigenwertzerlegung</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.2.</strong> Singulärwertzerlegung</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.3.</strong> Hauptkomponentenanalyse</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.4.</strong> Hauptkoordinatenanalyse</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.5.</strong> Lineare Gleichungssysteme</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Maschinelles Lernen</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">4.1.</strong> Überwachtes Lernen</div></li><li class="chapter-item "><div><strong aria-hidden="true">4.2.</strong> Unüberwachtes Lernen</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Neuronale Netzwerke</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">5.1.</strong> Single-Layer-Perzeptron</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.2.</strong> Multi-Layer-Perzeptron</div></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded affix "><div>Zusammenfassung und Ausblick</div></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded affix "><a href="psets/01.html">Problem Set 1</a></li><li class="chapter-item expanded affix "><div>Problem Set 2</div></li><li class="chapter-item expanded affix "><div>Problem Set 3</div></li><li class="chapter-item expanded affix "><div>Problem Set 4</div></li><li class="chapter-item expanded affix "><div>Problem Set 5</div></li><li class="chapter-item expanded affix "><div>Problem Set 6</div></li><li class="chapter-item expanded affix "><div>Sample Exam</div></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Programmierkurs für Chemiker</h1>

                    <div class="right-buttons">
			<!--print button-->
			<a href="../print.html" title="Buch drucken" aria-label="Buch drucken">
			    <i id="print-button" class="fa fa-print"></i>
			</a>

                        
                        <!-- WueCampus button -->
                        <script>
                            var wueCampusLink = "https://wuecampus.uni-wuerzburg.de/moodle/course/view.php?id=73209";
                        </script>
			<a href="javascript:void(0);" onclick="window.open(wueCampusLink)" target="_blank" rel="noopener noreferrer" title="WueCampus-Kursraum besuchen" aria-label="WueCampus-Kursraum besuchen">
			    <i id="wuecampus-button" class="icon-uw"></i>
			</a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h2 id="numerical-optimisation"><a class="header" href="#numerical-optimisation">Numerical Optimisation</a></h2>
<p>Numerical optimisation is a powerful tool that allows us to find the
minimum or maximum of complex functions, such as the loss functions of
least squares (Eq. <a href="01-least_squares.html#eq:least_squares_loss">(1.3)</a>), but also functions
in other contexts, such as the energy of a molecule. Since maximising a
function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> is equivalent to minimising <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, we will only speak of
minimisation in the following. The ability to tackle optimisation
problems for which no closed-form solutions exist, or where the
evaluation of the analytical expression is too expensive, significantly
expands our toolkit in data analysis. In particular, it allows us to
find solutions for models characterised by nonlinearities, high
dimensionalities, or unusual data distributions.</p>
<h3 id="theoretical-foundations"><a class="header" href="#theoretical-foundations">Theoretical Foundations</a></h3>
<p>A particularly accessible and fundamental approach to numerical
optimisation is the
<a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>
method. This is an <em>iterative method</em> that starts from a given initial point
<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span> and follows the direction of steepest descent of the function at
each step. Mathematically, this is formulated as
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8991em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9824em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:gradient_descent"></span></span><span class="tag"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.10</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> is the estimate of the minimum at step <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>. The superscript
<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> has nothing to do with exponentiation, but is merely a notation
to avoid confusion with the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th component of the vector, which is
subscripted here. The gradient of the <em>objective function</em> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> at <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span>
can then be noted as
<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8543em;vertical-align:-0.65em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.6014em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8408em;"><span style="top:-2.1885em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6166em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.6426em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.782em;"><span style="top:-2.214em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5576em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.2043em;"><span style="top:-3.6029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord amsrm mtight">⊺</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>The proportionality constant <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is called the <em>step size</em> or <em>learning
rate</em>. The procedure is repeated until one or more stopping
conditions are met. Typical stopping conditions for iterative
optimisation methods are:</p>
<ul>
<li>The change in the function value is smaller than a threshold</li>
<li>The change in the estimate is smaller than a threshold</li>
<li>A maximum number of iterations is reached</li>
<li>The norm of the gradient is smaller than a threshold.</li>
</ul>
<p>The step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is the only and at the same time an important
parameter of gradient descent, as it influences the convergence speed
and stability of the method. A too small value for <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> can lead
to the method converging very slowly, while a too large value can
lead to divergence.</p>
<p>To use gradient descent, we need access to the gradient of the
objective function. If this is not available analytically, a numerical
approximation must be used. A simple approach is the method of
<a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>.
Here, the tangent of the partial derivative is replaced by the secant,
which leads to an approximation of the form
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3692em;vertical-align:-0.9978em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.2791em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9978em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2121em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">h</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:finite_difference_symmetric"></span></span><span class="tag"><span class="strut" style="height:2.5239em;vertical-align:-0.9978em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.11</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th unit vector and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> is a small value
that determines the step size of the approximation. More precisely,
this equation <a href="#eq:finite_difference_symmetric">(1.11)</a> represents the
<em>central finite difference of order 2</em>. There are also <em>one-sided</em>
approximations and higher-order approximations, which we will not
discuss further here.</p>
<h3 id="implementation"><a class="header" href="#implementation">Implementation</a></h3>
<h4 id="finite-differences"><a class="header" href="#finite-differences">Finite Differences</a></h4>
<p>The first step is to implement the finite difference. Since we will
need to calculate derivatives multiple times, it is useful to
implement a <em>function</em>. Functions in the programming context are
similar to mathematical functions that transform an input into an
output. However, they can do much more.</p>
<p>We first import <code>numpy</code>:</p>
<pre><code class="language-python">import numpy as np
</code></pre>
<p>Then we define the function <code>finite_difference</code>, which calculates the
gradient of a function <code>func</code> at the point <code>x0</code>.</p>
<pre><code class="language-python">def finite_difference(func, x0, h=1e-5, args=()):
    n = len(x0)
    grad = np.zeros(n)

    for i in range(0, n):
        e = np.zeros(n)
        e[i] = 1
        grad[i] = (
            func(x0 + h * e, *args) - func(x0 - h * e, *args)
        ) / (2 * h)

    return grad
</code></pre>
<p>The first block of the function is the <em>signature</em>, which indicates
which arguments the function expects. In strongly typed languages,
function signatures also define the types of the arguments and the
return type. Because this is not the case in Python, the naming
for the arguments becomes important.</p>
<p>The signature of the function <code>finite_difference</code> indicates that it
expects the argument <code>func</code>, which is probably a function from its name.
The next argument is <code>x0</code>, which is named inline with the mathematical
notation. The next two arguments are <code>h</code> and <code>args</code>, which are optional.
This can be seen in the signature, where the default values are defined
as <code>h=1e-5</code> and <code>args=()</code>. The default value for <code>h</code> is a small
positive number, indicating that could be the step size, and the default
value for <code>args</code> is an empty tuple, indicating that it could be used
to pass additional arguments to the function <code>func</code>.</p>
<p>In the implementation of the function, we first determine the dimension
of the point <code>x0</code> and store it in the integer <code>n</code>. The gradient of a
real-valued function with <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> variables is a vector of length <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>.
Therefore, the variable <code>grad</code> is initialised as a numpy array of
length <code>n</code> filled with zeros using the function
<a href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html"><code>np.zeros</code></a>.</p>
<p>Because we need to apply Eq. <a href="#eq:finite_difference_symmetric">(1.11)</a> to all
<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> components of the point <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span>, it is useful to use a <em>loop</em>.
Here we use a
<a href="https://docs.python.org/3/tutorial/controlflow.html#for-statements"><em>for</em>-loop</a>
that iterates over all indices <code>i</code> from <code>0</code> to <code>n - 1</code>. Note that
Python indices start at <code>0</code> and the built-in function
<a href="https://docs.python.org/3/library/functions.html#func-range"><code>range</code></a>
excludes the end value <code>n</code>.</p>
<p>In each iteration of the loop, we first define the unit vector
<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> by first creating a zero array of length <code>n</code> with
<code>np.zeros(n)</code> and then setting the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th component to <code>1</code>.
Then we can calculate the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th entry of the gradient array <code>grad</code>
according to Eq. <a href="#eq:finite_difference_symmetric">(1.11)</a>. The only
difference between our code and this equation is that we pass the
additional argument <code>args</code> to the function <code>func</code>, with a star <code>*</code>
in front of the argument <code>args</code>. The use of <code>*</code> acts as a unary
operator at this point, which unpacks an object into its components
(see <a href="https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists"><em>unpacking</em></a>).
This means that the function <code>func</code> accepts the further arguments
in the tuple <code>args</code> <strong>individually</strong> after the first argument.</p>
<p>After the last iteration, we return the gradient in the variable <code>grad</code>.</p>
<h4 id="objective-function"><a class="header" href="#objective-function">Objective Function</a></h4>
<p>Next, we implement the objective function whose value we want to
minimise. According to Eq. <a href="02-linear_regression.html#eq:least_squares_loss_linear">(1.8)</a>
we can define the function <code>objective_function</code> as follows:</p>
<pre><code class="language-python">def objective_function(beta, *args):
    concentrations = args[0]
    absorbances = args[1]
    return np.sum((absorbances - (beta[0] + beta[1] * concentrations))**2)
</code></pre>
<p>The first argument in this function is <code>beta</code>, which are the parameters
to be optimised. The second argument is <code>args</code>, which contains the
additional arguments for the objective function. Here we see the use
of <code>*</code> in front of the argument <code>args</code> again, which makes the function
<code>objective_function</code> accept any number of additional arguments after
<code>beta</code>.</p>
<p>In the implementation of the function <code>objective_function</code>, we first
define the arrays <code>concenctrations</code> and <code>absorbances</code> from the argument
<code>args</code> and then return the value of the loss function of least
squares according to Eq. <a href="02-linear_regression.html#eq:least_squares_loss_linear">(1.8)</a>.
There are no big differences between the mathematical formulation and the
programmatic implementation here.</p>
<p>With the help of the function <code>finite_difference</code>, we can implement
the function <code>objective_function_gradient</code>, which calculates the
gradient of the objective function:</p>
<pre><code class="language-python">def objective_function_gradient(beta, *args):
    concentrations = args[0]
    absorbances = args[1]
    grad = finite_difference(
        objective_function, 
        beta, 
        args=(concentrations, absorbances),
    )
    return grad
</code></pre>
<p>This function takes the same arguments as the function
<code>objective_function</code>, but returns the gradient of the objective
function instead of its value. In its implementation, we again
define the arrays <code>concenctrations</code> and <code>absorbances</code> from the
argument <code>args</code>, and then call the function <code>finite_difference</code> to calculate
the gradient of the objective function. The result is stored in the
variable <code>grad</code> and returned.</p>
<h4 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient Descent</a></h4>
<p>Now we can implement the gradient descent algorithm. According to
Eq. <a href="#eq:gradient_descent">(1.10)</a> we need the gradient of the objective
function <code>func_grad</code>, the starting point <code>x0</code>, and the step size
<code>alpha</code> to start the algorithm. As a stopping condition, we use a
combination of the maximum number of iterations and the norm of
the gradient. This leads to the additional arguments <code>max_iter</code> and
<code>max_norm</code>. We also need the argument <code>args</code>, which contains the
additional arguments for the function <code>func_grad</code>.</p>
<pre><code class="language-python">def gradient_descent(func_grad, x0, alpha=0.001, 
                     max_norm=1e-6, max_iter=10000, args=()):
    x = np.copy(x0)
    for niter in range(0, max_iter):
        grad = func_grad(x, *args)
        x = x - alpha * grad
        if np.linalg.norm(grad) &lt; max_norm:
            break
    if niter == max_iter - 1:
        print('Warning: Maximum iterations reached. '
              'Result may not be reliable.')
    
    return x, niter
</code></pre>
<p>First, we explicitly copy the array <code>x0</code> with the function <code>np.copy</code>
and store the copy in the variable <code>x</code>. This is necessary because
Python does not copy arrays by default, but only stores references
to them. This means that a change in <code>x0</code> will cause a change in <code>x</code>,
and <em>vice versa</em>.</p>
<p>Then we use a <code>for</code>-loop that iterates the variable <code>niter</code> from <code>0</code> to
<code>max_iter - 1</code>. In each iteration, we calculate the gradient <code>grad</code>
by calling the function <code>func_grad</code> with the arguments <code>x</code> and <code>args</code>.
Then we use Eq. <a href="#eq:gradient_descent">(1.10)</a> to update the variable
<code>x</code>. After that, we calculate the norm of the gradient with the
function
<a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code>np.linalg.norm</code></a>
and check if it is smaller than <code>max_norm</code>. If so, we break the
loop with the <code>break</code> command.</p>
<p>After the last iteration, we check if the variable <code>niter</code> has reached
<code>max_iter</code>. If so, it means that the method may not have converged,
and we issue a warning. At the end, we return the optimum <code>x</code> and
the number of iterations <code>niter</code> that were actually needed.</p>
<h3 id="application"><a class="header" href="#application">Application</a></h3>
<p>Now we can apply the implemented algorithm to the data from Chapter
<a href="02-linear_regression.html">1.2</a>. First, we define the arrays
<code>concentrations</code> and <code>absorbances</code> again:</p>
<pre><code class="language-python">concentrations = [
    2.125, 4.250, 6.375, 8.500, 10.63, 12.75, 14.88, 17.00, 19.13, 21.25,
    23.38, 25.50, 27.63, 29.75, 31.88, 34.00, 36.13, 38.25, 40.38, 42.50,
]
absorbances = [
    0.0572, 0.1391, 0.2049, 0.2754, 0.3420, 
    0.4139, 0.4956, 0.5815, 0.6806, 0.7481,
    0.8242, 0.9130, 1.0043, 1.0809, 1.1511,
    1.2483, 1.3373, 1.4027, 1.4927, 1.5853,
]
concentrations = np.array(concentrations)
absorbances = np.array(absorbances)
</code></pre>
<p>Then we define the initial point <code>x0</code>, here <code>beta_guess</code>, and call the
function <code>gradient_descent</code>:</p>
<pre><code class="language-python">beta_guess = np.array([1.0, 1.0])
beta_opt, niter = gradient_descent(
    objective_function_gradient, 
    beta_guess,
    alpha=0.00005,
    max_iter=100000,
    args=(concentrations, absorbances),
)

beta0, beta1 = beta_opt
assert np.isclose(beta0, -0.04907034)
assert np.isclose(beta1, 0.03800109)

print(beta_opt)
print(niter)
</code></pre>
<p>The optimal parameters <code>beta0</code> and <code>beta1</code> are identical to those of the
analytical solution. On the author’s computer, 34683 iterations were
needed to satisfy the stopping condition. The exact number of
iterations may vary slightly depending on the hardware. If you
choose the step size <code>alpha</code> slightly larger, fewer iterations are
needed. However, if <code>alpha</code> is too large, the method diverges.</p>
<div id="admonition-tip" class="admonition admonish-tip" role="note" aria-labelledby="admonition-tip-title">
<div class="admonition-title">
<div id="admonition-tip-title">
<p>Tip</p>
</div>
<a class="admonition-anchor-link" href="#admonition-tip"></a>
</div>
<div>
<p>Try changing the step size <code>alpha</code> and observe how the number of
iterations changes.</p>
</div>
</div>
<p>Since optimisation is a very general problem, there are many implementations
of various algorithms in libraries such as
<a href="https://docs.scipy.org/doc/scipy/reference/optimize.html"><code>scipy.optimize</code></a>.
We want to use the function
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code>scipy.optimize.minimize</code></a>
to find the optimal parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>:</p>
<pre><code class="language-python">from scipy.optimize import minimize

res = minimize(
    objective_function,
    beta_guess,
    args=(concentrations, absorbances),
    method='CG',
    jac=objective_function_gradient,
    options={'maxiter': 10000, 'gtol': 1e-6},
)

beta0, beta1 = res.x
niter = res.nit
assert np.isclose(beta0, -0.04907034)
assert np.isclose(beta1, 0.03800109)
</code></pre>
<p>When calling the function <code>minimize</code>, we only need to specify the
objective function <code>objective_function</code> and the initial point <code>beta_guess</code>.
The <code>minimize</code> function takes care of the numerical gradient
calculation itself.</p>
<div id="admonition-note-on-the-minimize-function" class="admonition admonish-note" role="note" aria-labelledby="admonition-note-on-the-minimize-function-title">
<div class="admonition-title">
<div id="admonition-note-on-the-minimize-function-title">
<p>Note on the <code>minimize</code> function</p>
</div>
<a class="admonition-anchor-link" href="#admonition-note-on-the-minimize-function"></a>
</div>
<div>
<p>The <code>minimize</code> function also accepts the argument <code>jac</code>
(<a href="https://en.wikipedia.org/wiki/Jacobian_matrix">Jacobian matrix</a>), i.e. a
function that calculates the gradient of the objective function. If you
have the analytical and easily computable gradient available, you can
pass it as the <code>jac</code> argument, which can speed up the optimisation
process.</p>
</div>
</div>
<p>With the argument <code>method='CG'</code>, we have selected the
<a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method"><i>nonlinear <strong>C</strong>onjugate <strong>G</strong>radient method</i></a>.
This method is an improvement of the gradient method and reaches the
minimum in only 2 iterations on the author’s computer.</p>
<p>In <code>minimize</code>, there are a number of other minimisation methods that
can be used. An overview can be found in the
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">documentation</a>
of this function. Two important methods among them are:</p>
<ul>
<li><code>method='Nelder-Mead'</code>:
The <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead method</a>
is a heuristic method that does not require the calculation of the
gradient. It is therefore particularly useful when the calculation
of the gradient is very expensive or the gradient is noisy. It
is especially suitable for regressions with experimental data.</li>
<li><code>method='BFGS'</code>:
The <a href="https://en.wikipedia.org/wiki/BFGS_method"><strong>B</strong>royden-<strong>F</strong>letcher-<strong>G</strong>oldfarb-<strong>S</strong>hanno method</a>
is a method that uses the gradient to approximate the Hesse matrix
(Hessian) of the objective function. The Hessian contains the
second derivatives of the function with respect to its parameters.
The method therefore shows very fast convergence near the minimum.
In practice, it requires fewer iterations than other optimisation
methods and is therefore often used.</li>
</ul>
<p>As you will see in the exercise, the linear regression with the least
squares method has a closed-form solution that directly calculates the optimal
parameters. So why should we bother with numerical optimisation?
In the context of regression, numerical optimisation allows us to
use more complicated models that do not have an analytical solution,
and also to use more sophisticated loss functions, such as the
least absolute deviations (see Eq. <a href="01-least_squares.html#eq:least_absolute_deviations_opt">(1.6)</a>).
In addition, we can introduce additional control over the parameters
(<strong>regularisation</strong>), which can improve the general performance of the
model.</p>
<div id="admonition-function-scipyoptimizecurve_fit" class="admonition admonish-note" role="note" aria-labelledby="admonition-function-scipyoptimizecurve_fit-title">
<div class="admonition-title">
<div id="admonition-function-scipyoptimizecurve_fit-title">
<p>Function <code>scipy.optimize.curve_fit</code></p>
</div>
<a class="admonition-anchor-link" href="#admonition-function-scipyoptimizecurve_fit"></a>
</div>
<div>
<p>There is also the function
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code>scipy.optimize.curve_fit</code></a>,
which performs a (nonlinear) regression of the data directly.
However, it is not as flexible as the general method with the function
<code>minimize</code>, as it only has a few optimisation methods and sets the
objective function as the least squares loss function.
It can be applied to simple regressions and usually requires less code.</p>
</div>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../01-regression/02-linear_regression.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../psets/01.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../01-regression/02-linear_regression.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../psets/01.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src=".././theme/pagetoc.js"></script>


    </div>
    </body>
</html>
