<!DOCTYPE HTML>
<html lang="de" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Numerical Optimisation - Programmierkurs für Chemiker</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">
        <link rel="stylesheet" href=".././theme/catppuccin-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish.css">
        <link rel="stylesheet" href=".././theme/mdbook-admonish-custom.css">
        <link rel="stylesheet" href=".././theme/icomoon.css">
        <link rel="stylesheet" href=".././theme/pagetoc.css">
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "mocha" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Inhaltsverzeichnis</li><li class="chapter-item expanded "><a href="../00-preface.html"><strong aria-hidden="true">0.</strong> Vorwort</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../00-preface/01-motivation.html"><strong aria-hidden="true">0.1.</strong> Motivation</a></li><li class="chapter-item "><a href="../00-preface/02-getting_started.html"><strong aria-hidden="true">0.2.</strong> Erste Schritte</a></li><li class="chapter-item "><a href="../00-preface/03-mdbook_usage.html"><strong aria-hidden="true">0.3.</strong> Bedienung dieser Website</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../01-regression.html"><strong aria-hidden="true">1.</strong> Regression Analysis</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../01-regression/01-least_squares.html"><strong aria-hidden="true">1.1.</strong> Least Squares</a></li><li class="chapter-item "><a href="../01-regression/02-linear_regression.html"><strong aria-hidden="true">1.2.</strong> Linear Regression</a></li><li class="chapter-item expanded "><a href="../01-regression/03-numerical_optimisation.html" class="active"><strong aria-hidden="true">1.3.</strong> Numerical Optimisation</a></li><li class="chapter-item "><a href="../01-regression/04-nonlinear_regression.html"><strong aria-hidden="true">1.4.</strong> Nonlinear Regression</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> Differentialgleichungen</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">2.1.</strong> Anfangswertproblem</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.2.</strong> Euler-Verfahren</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.3.</strong> Runge-Kutta-Verfahren</div></li><li class="chapter-item "><div><strong aria-hidden="true">2.4.</strong> Finite-Differenzen-Verfahren</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> Eigenwert- und Singulärwertzerlegung</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">3.1.</strong> Eigenwertzerlegung</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.2.</strong> Singulärwertzerlegung</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.3.</strong> Hauptkomponentenanalyse</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.4.</strong> Hauptkoordinatenanalyse</div></li><li class="chapter-item "><div><strong aria-hidden="true">3.5.</strong> Lineare Gleichungssysteme</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Maschinelles Lernen</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">4.1.</strong> Überwachtes Lernen</div></li><li class="chapter-item "><div><strong aria-hidden="true">4.2.</strong> Unüberwachtes Lernen</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Neuronale Netzwerke</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">5.1.</strong> Single-Layer-Perzeptron</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.2.</strong> Multi-Layer-Perzeptron</div></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded affix "><div>Zusammenfassung und Ausblick</div></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded affix "><a href="../psets/01.html">Problem Set 1</a></li><li class="chapter-item expanded affix "><div>Problem Set 2</div></li><li class="chapter-item expanded affix "><div>Problem Set 3</div></li><li class="chapter-item expanded affix "><div>Problem Set 4</div></li><li class="chapter-item expanded affix "><div>Problem Set 5</div></li><li class="chapter-item expanded affix "><div>Problem Set 6</div></li><li class="chapter-item expanded affix "><div>Sample Exam</div></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Programmierkurs für Chemiker</h1>

                    <div class="right-buttons">
			<!--print button-->
			<a href="../print.html" title="Buch drucken" aria-label="Buch drucken">
			    <i id="print-button" class="fa fa-print"></i>
			</a>

                        
                        <!-- WueCampus button -->
                        <script>
                            var wueCampusLink = "https://wuecampus.uni-wuerzburg.de/moodle/course/view.php?id=73209";
                        </script>
			<a href="javascript:void(0);" onclick="window.open(wueCampusLink)" target="_blank" rel="noopener noreferrer" title="WueCampus-Kursraum besuchen" aria-label="WueCampus-Kursraum besuchen">
			    <i id="wuecampus-button" class="icon-uw"></i>
			</a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h2 id="numerical-optimisation"><a class="header" href="#numerical-optimisation">Numerical Optimisation</a></h2>
<p>In many scientific problems, we are interested in finding the global minimum or maximum of a function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, or the parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> that yield this minimum or maximum. For example, in the previous chapter, we wanted to find the parameters that minimise the loss function, and in many applications in quantum chemistry, we want to find the coefficients that minimise the energy of a molecule. Since maximising a function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> is equivalent to minimising <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, we will only speak of minimisation in the following. Because no analytical solution exists for most of these problems, the question arises how we can find the global minimum or maximum of a function numerically.</p>
<p>In the vast majority of cases, solving this problem is simply impossible due to the fact that the given function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> is highly non-convex (i.e. it has many local minima) or is too high-dimensional (i.e. it has many parameters). However, there are a number of methods that allow us to find a <strong>local minimum</strong> of a function, such as the gradient descent method.</p>
<h3 id="theoretical-foundations"><a class="header" href="#theoretical-foundations">Theoretical Foundations</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is an <em>iterative method</em> that starts from a given initial set of parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span> and follows the direction of steepest descent of the function <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> at each step. Mathematically, one step of the algorithm is formulated as
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8991em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9824em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:gradient_descent"></span></span><span class="tag"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.10</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> is the estimate of the minimum at step <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>. The superscript <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> has nothing to do with exponentiation, but is merely a notation to avoid confusion with the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th component of the vector, which is subscripted here. The gradient of the <em>objective function</em> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> at <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> can then be noted as <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8543em;vertical-align:-0.65em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.6014em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8408em;"><span style="top:-2.1885em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6166em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.6426em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.782em;"><span style="top:-2.214em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5576em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.2043em;"><span style="top:-3.6029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord amsrm mtight">⊺</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>The proportionality constant <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is called the <em>step size</em> or <em>learning rate</em>. The procedure is repeated until one or more stopping conditions are met. Typical stopping conditions for iterative optimisation methods are:</p>
<ul>
<li>The change in the function value is smaller than a threshold</li>
<li>The change in the parameters is smaller than a threshold</li>
<li>A maximum number of iterations is reached</li>
<li>The norm of the gradient is smaller than a threshold.</li>
</ul>
<p>The step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is the only and at the same time an important parameter of gradient descent, as it influences the convergence speed and stability of the method. A too small value for <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> can lead to the method converging very slowly, while a too large value can lead to divergence.</p>
<p>To use gradient descent, we need access to the gradient of the objective function with respect to the parameters. If this is not available analytically, a numerical approximation must be used. A simple approach is the method of <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>. Here, the tangent of the partial derivative is replaced by the secant, which leads to an approximation of the form
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3692em;vertical-align:-0.9978em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.2791em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em;"><span style="top:-2.4231em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9978em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2121em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">h</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="enclosing" id="eq:finite_difference_symmetric"></span></span><span class="tag"><span class="strut" style="height:2.5239em;vertical-align:-0.9978em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1.11</span></span><span class="mord">)</span></span></span></span></span></span>
where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th unit vector and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> is a small value that determines the step size of the approximation. More precisely, this equation <a href="#eq:finite_difference_symmetric">(1.11)</a> represents the <em>central finite difference of order 2</em>. There are also <em>one-sided</em> approximations and higher-order approximations, which we will not discuss further here.</p>
<h3 id="implementation"><a class="header" href="#implementation">Implementation</a></h3>
<h4 id="finite-differences"><a class="header" href="#finite-differences">Finite Differences</a></h4>
<p>The first step is to implement the finite difference. Since we will need to calculate derivatives multiple times, it is useful to implement a <em>function</em>. Functions in the programming context are similar to mathematical functions that transform an input into an output, given a set of rules. In contrast to the code blocks that we have seen so far, the code that defines a function is not executed immediately. Instead, it is only executed when the function is called. Let’s see how this works in practice.</p>
<p>We first import <code>numpy</code>:</p>
<pre><code class="language-python">import numpy as np
</code></pre>
<p>Then we define the function <code>finite_difference</code>, which should calculate the gradient of a function <code>func</code> at the point <code>theta0</code>.</p>
<pre><code class="language-python">def finite_difference(func, theta0, h=1e-5, args=()):
    n = len(theta0)
    grad = np.zeros(n)

    for i in range(0, n):
        e = np.zeros(n)
        e[i] = 1
        grad[i] = (
            func(theta0 + h * e, *args) - func(theta0 - h * e, *args)
        ) / (2 * h)

    return grad
</code></pre>
<p>The first line of the function is the <em>signature</em>, which defines the name of the function and the names and types of the arguments.</p>
<p>The signature of the function <code>finite_difference</code> indicates that it expects the argument <code>func</code>, which represents the function to be differentiated. The next argument is the array <code>theta0</code>, which represents the point at which the gradient is to be calculated. The next two arguments are <code>h</code> and <code>args</code>, which are optional. This can be seen in the signature, where the default values are defined as <code>h=1e-5</code> and <code>args=()</code>. The default value for <code>h</code> is a small positive number, indicating that could be the step size, and the default value for <code>args</code> is an empty tuple, indicating that it could be used to pass additional arguments to the function <code>func</code>.</p>
<div id="admonition-dynamic-typing-in-python" class="admonition admonish-note">
<div class="admonition-title">
<p>Dynamic typing in Python</p>
<p><a class="admonition-anchor-link" href="#admonition-dynamic-typing-in-python"></a></p>
</div>
<div>
<p>In strongly typed languages, function signatures also define the types of the arguments and the return type. Since Python is a dynamically typed language, the types of the arguments are only defined when the function is called. We therefore have to be careful in naming the arguments, such that the type of the argument is clear from the name.</p>
</div>
</div>
<p>In the body of the function (the code block that follows the signature), we first determine the dimension of the parameters <code>theta0</code> and store it in the integer <code>n</code>. The gradient of a real-valued function with <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> variables is a vector of length <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>. Therefore, the variable <code>grad</code> is initialised as a numpy array of length <code>n</code> filled with zeros using the function <a href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html"><code>np.zeros</code></a>. We will use this array to store the gradient of the function.</p>
<p>Because we need to apply Eq. <a href="#eq:finite_difference_symmetric">(1.11)</a> to all <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> components of the point <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> individually, it is useful to do this in a <em>loop</em>. Here we use a <a href="https://docs.python.org/3/tutorial/controlflow.html#for-statements"><em>for</em>-loop</a> that iterates over all dimensions <code>i</code> from <code>0</code> to <code>n - 1</code>. Note that Python indices start at <code>0</code> and the built-in function <a href="https://docs.python.org/3/library/functions.html#func-range"><code>range</code></a> excludes the end value <code>n</code>.</p>
<p>In each iteration of the loop, we first define the unit vector <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> by first creating a zero array of length <code>n</code> with <code>np.zeros(n)</code> and then setting the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th component to <code>1</code>. We then can calculate the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>-th entry of the gradient array <code>grad</code> according to Eq. <a href="#eq:finite_difference_symmetric">(1.11)</a>. The only difference between our code and this equation is that we pass the additional argument <code>args</code> to the function <code>func</code>, with a star <code>*</code> in front of the argument <code>args</code>. The use of <code>*</code> acts as a unary operator at this point, which unpacks an object into its components (see <a href="https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists"><em>unpacking</em></a>). This means that the function <code>func</code> accepts the further arguments in the tuple <code>args</code> <strong>individually</strong> after the first argument.</p>
<p>After the completion of the loop, we return the gradient in the variable <code>grad</code>, which is a numpy array of length <code>n</code>.</p>
<h4 id="objective-function"><a class="header" href="#objective-function">Objective Function</a></h4>
<p>Next, we implement the function whose value we want to minimise. This can be any function, but we choose the least squares loss function <a href="02-linear_regression.html#eq:least_squares_loss_linear">(1.8)</a> of the linear regression model, which we can implement as follows:</p>
<pre><code class="language-python">def least_squares_loss(beta, x, y):
    return np.sum((y - (beta[0] + beta[1] * x))**2)
</code></pre>
<p>Inline with our previous notation, we use the variable <code>beta</code> to represent the parameters, which we will later optimise. The second and third arguments are <code>x</code> and <code>y</code>, which are the data points.</p>
<p>The gradient of the least squares loss function with respect to the parameters can now easily be calculated using the function <code>finite_difference</code> that we implemented earlier. But first, let’s implement the gradient descent algorithm.</p>
<h4 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient Descent</a></h4>
<p>According to Eq. <a href="#eq:gradient_descent">(1.10)</a>, the algorithm requires the gradient of the objective function, the initial parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span>, the step size <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>, and the maximum number of iterations. Instead of the gradient, we can also pass the function directly to the algorithm and estimate the gradient numerically using the <code>finite_difference</code> function. As a stopping condition, we use a combination of the maximum number of iterations and the norm of the gradient. Therefore, we need the additional arguments <code>max_iter</code> and <code>max_norm</code>. We also need the argument <code>args</code>, which contains the additional arguments for the function that we want to optimise.</p>
<pre><code class="language-python">def gradient_descent(func, theta0, alpha=0.001, 
                     max_norm=1e-6, max_iter=10000, args=()):
    theta = np.copy(theta0)
    for niter in range(0, max_iter):
        grad = finite_difference(func, theta, args=args)
        theta = theta - alpha * grad
        if np.linalg.norm(grad) &lt; max_norm:
            break
    if niter == max_iter - 1:
        print('Warning: Maximum iterations reached. '
              'Result may not be reliable.')
    
    return theta, niter
</code></pre>
<p>In the body of the function, we first explicitly copy the array <code>theta0</code> with the function <code>np.copy</code> and store the copy in the variable <code>theta</code>. This is necessary because Python does not copy arrays by default, but only stores references to them. This means that when simply renaming <code>theta0</code> to <code>theta</code>, a change in <code>theta0</code> will also cause a change in <code>theta</code>, and <em>vice versa</em>.</p>
<p>Then we use a <code>for</code>-loop that iterates the variable <code>niter</code> from <code>0</code> to <code>max_iter - 1</code>. In each iteration, we calculate the gradient <code>grad</code> by calling the function <code>func_grad</code> with the arguments <code>x</code> and <code>args</code>. Then we use Eq. <a href="#eq:gradient_descent">(1.10)</a> to update the variable <code>x</code>. After that, we calculate the norm of the gradient with the function <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code>np.linalg.norm</code></a> and check if it is smaller than <code>max_norm</code>. If so, we break the loop with the <code>break</code> command.</p>
<p>After the last iteration, we check if the variable <code>niter</code> has reached
<code>max_iter</code>. If so, it means that the method may not have converged,
and we issue a warning. At the end, we return the optimum <code>x</code> and
the number of iterations <code>niter</code> that were actually needed.</p>
<h3 id="application"><a class="header" href="#application">Application</a></h3>
<p>Now we can apply the implemented algorithm to the data from Chapter
<a href="02-linear_regression.html">1.2</a>. First, we define the arrays
<code>concentrations</code> and <code>absorbances</code> again:</p>
<pre><code class="language-python">concentrations = [
    2.125, 4.250, 6.375, 8.500, 10.63, 12.75, 14.88, 17.00, 19.13, 21.25,
    23.38, 25.50, 27.63, 29.75, 31.88, 34.00, 36.13, 38.25, 40.38, 42.50,
]
absorbances = [
    0.0572, 0.1391, 0.2049, 0.2754, 0.3420, 
    0.4139, 0.4956, 0.5815, 0.6806, 0.7481,
    0.8242, 0.9130, 1.0043, 1.0809, 1.1511,
    1.2483, 1.3373, 1.4027, 1.4927, 1.5853,
]
concentrations = np.array(concentrations)
absorbances = np.array(absorbances)
</code></pre>
<p>Then we define the initial point <code>x0</code>, here <code>beta_guess</code>, and call the
function <code>gradient_descent</code>:</p>
<pre><code class="language-python">beta_guess = np.array([1.0, 1.0])
beta_opt, niter = gradient_descent(
    least_squares_loss, 
    beta_guess,
    alpha=0.00005,
    max_iter=100000,
    args=(concentrations, absorbances),
)

beta0, beta1 = beta_opt
assert np.isclose(beta0, -0.04907034)
assert np.isclose(beta1, 0.03800109)

print(beta_opt)
print(niter)
</code></pre>
<p>The optimal parameters <code>beta0</code> and <code>beta1</code> are identical to those of the
analytical solution. On the author’s computer, 34683 iterations were
needed to satisfy the stopping condition. The exact number of
iterations may vary slightly depending on the hardware. If you
choose the step size <code>alpha</code> slightly larger, fewer iterations are
needed. However, if <code>alpha</code> is too large, the method diverges.</p>
<div id="admonition-tip" class="admonition admonish-tip">
<div class="admonition-title">
<p>Tip</p>
<p><a class="admonition-anchor-link" href="#admonition-tip"></a></p>
</div>
<div>
<p>Try changing the step size <code>alpha</code> and observe how the number of
iterations changes.</p>
</div>
</div>
<p>Since optimisation is a very general problem, there are many implementations
of various algorithms in libraries such as
<a href="https://docs.scipy.org/doc/scipy/reference/optimize.html"><code>scipy.optimize</code></a>.
We want to use the function
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code>scipy.optimize.minimize</code></a>
to find the optimal parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>:</p>
<pre><code class="language-python">from scipy.optimize import minimize

res = minimize(
    least_squares_loss,
    beta_guess,
    args=(concentrations, absorbances),
    method='CG',
    jac=lambda beta, x, y: finite_difference(least_squares_loss, beta, args=(x, y)),
    options={'maxiter': 10000, 'gtol': 1e-6},
)

beta0, beta1 = res.x
niter = res.nit
assert np.isclose(beta0, -0.04907034)
assert np.isclose(beta1, 0.03800109)
</code></pre>
<p>When calling the function <code>minimize</code>, we only need to specify the
objective function <code>objective_function</code> and the initial point <code>beta_guess</code>.
The <code>minimize</code> function takes care of the numerical gradient
calculation itself.</p>
<div id="admonition-note-on-the-minimize-function" class="admonition admonish-note">
<div class="admonition-title">
<p>Note on the <code>minimize</code> function</p>
<p><a class="admonition-anchor-link" href="#admonition-note-on-the-minimize-function"></a></p>
</div>
<div>
<p>The <code>minimize</code> function also accepts the argument <code>jac</code>
(<a href="https://en.wikipedia.org/wiki/Jacobian_matrix">Jacobian matrix</a>), i.e. a
function that calculates the gradient of the objective function. If you
have the analytical and easily computable gradient available, you can
pass it as the <code>jac</code> argument, which can speed up the optimisation
process.</p>
</div>
</div>
<p>With the argument <code>method='CG'</code>, we have selected the
<a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method"><i>nonlinear <strong>C</strong>onjugate <strong>G</strong>radient method</i></a>.
This method is an improvement of the gradient method and reaches the
minimum in only 2 iterations on the author’s computer.</p>
<p>In <code>minimize</code>, there are a number of other minimisation methods that
can be used. An overview can be found in the
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">documentation</a>
of this function. Two important methods among them are:</p>
<ul>
<li><code>method='Nelder-Mead'</code>:
The <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead method</a>
is a heuristic method that does not require the calculation of the
gradient. It is therefore particularly useful when the calculation
of the gradient is very expensive or the gradient is noisy. It
is especially suitable for regressions with experimental data.</li>
<li><code>method='BFGS'</code>:
The <a href="https://en.wikipedia.org/wiki/BFGS_method"><strong>B</strong>royden-<strong>F</strong>letcher-<strong>G</strong>oldfarb-<strong>S</strong>hanno method</a>
is a method that uses the gradient to approximate the Hesse matrix
(Hessian) of the objective function. The Hessian contains the
second derivatives of the function with respect to its parameters.
The method therefore shows very fast convergence near the minimum.
In practice, it requires fewer iterations than other optimisation
methods and is therefore often used.</li>
</ul>
<p>As you will see in the exercise, the linear regression with the least
squares method has a closed-form solution that directly calculates the optimal
parameters. So why should we bother with numerical optimisation?
In the context of regression, numerical optimisation allows us to
use more complicated models that do not have an analytical solution,
and also to use more sophisticated loss functions, such as the
least absolute deviations (see Eq. <a href="01-least_squares.html#eq:least_absolute_deviations_opt">(1.6)</a>).
In addition, we can introduce additional control over the parameters
(<strong>regularisation</strong>), which can improve the general performance of the
model.</p>
<div id="admonition-function-scipyoptimizecurve_fit" class="admonition admonish-note">
<div class="admonition-title">
<p>Function <code>scipy.optimize.curve_fit</code></p>
<p><a class="admonition-anchor-link" href="#admonition-function-scipyoptimizecurve_fit"></a></p>
</div>
<div>
<p>There is also the function
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code>scipy.optimize.curve_fit</code></a>,
which performs a (nonlinear) regression of the data directly.
However, it is not as flexible as the general method with the function
<code>minimize</code>, as it only has a few optimisation methods and sets the
objective function as the least squares loss function.
It can be applied to simple regressions and usually requires less code.</p>
</div>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../01-regression/02-linear_regression.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../01-regression/04-nonlinear_regression.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../01-regression/02-linear_regression.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../01-regression/04-nonlinear_regression.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src=".././theme/pagetoc.js"></script>


    </div>
    </body>
</html>
