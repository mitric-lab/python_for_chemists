# Übung 6

## Aufgabe 1: Gradienten der logistischen Regression

<!--- ANCHOR: aufgabe_1 --->
Zeigen Sie, dass die Gradienten der Verlustfunktion der logistischen Regression {{eqref: eq:logistic_loss}} 
nach den Parametern $\vec{w}$ und $b$ gegeben sind durch

$$
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \vec{w}} &= - \sum_{i=1}^N \left[ y_i - \hat{f}(\vec{x}_i) \right] \vec{x}_i \\
    \frac{\partial \mathcal{L}}{\partial b} &= - \sum_{i=1}^N \left[ y_i - \hat{f}(\vec{x}_i) \right] .
\end{align}
$$

Zeigen Sie dazu (unter Benutzung der Kettenregel) zunächst, dass die Ableitung der Sigmoid-Funktion 
$\sigma(z) = \frac{1}{1 + \exp(-z)}$ gegeben ist durch 

$$
    \frac{d \sigma(z)}{dz} = \sigma(z) (1 - \sigma(z)).
$$

Vergleichen Sie die Gradienten der logistischen Regression mit den Gradienten der linearen Regression 
für die Verlustfunktion der Methode der kleinsten Quadrate. Was fällt Ihnen auf?
<!--- ANCHOR_END: aufgabe_1 --->

## Aufgabe 2: Binäre Kreuzentropie

<!--- ANCHOR: aufgabe_2 --->
Wie wir schon in der Vorlesung gesehen haben, ist die Verlustfunktion der Methode der kleinsten 
Quadrate keine geeignete Methode, um Klassifikationsprobleme zu lösen. Für binäre 
Klassifikationsprobleme wird daher in der Regel die binäre Kreuzentropie 

$$
    \mathcal{H} = -\frac{1}{N} \sum_{i=1}^N y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)
$$

als Verlustfunktion verwendet. Dabei ist $y_i \in \{0, 1\}$ das wahre Label des $i$-ten Datenpunkts, 
und $\hat{y}_i = \hat{f}(\vec{x}_i)$ die Vorhersage des Modells. Hier setzt die Verwendung des Logarithmus 
voraus, dass die Vorhersage des Modells auf das Intervall $(0, 1)$ abgebildet wird, 
was durch die Sigmoid-Funktion erreicht werden kann.

**(a)**

Modifizieren Sie die Implementierung des SLP anhand des *Circles*-Datensatzes, sodass die 
binäre Kreuzentropie als Verlustfunktion verwendet wird. Beachten Sie, dass Sie dazu die 
Gradienten der Verlustfunktion $\mathcal{H}$ nach den Parametern $\vec{w}$ und $b$ berechnen 
müssen, wobei Ihnen die Ergebnisse der vorherigen Aufgabe helfen können.

**(b)**

Wie ist die Verlustfunktion der negativen log likelihood {{eqref: eq:logistic_loss}} 
der logistischen Regression mit der (binären) Kreuzentropie verwandt?
<!--- ANCHOR_END: aufgabe_2 --->

## Aufgabe 3: Softmax und Kreuzentropie

<!--- ANCHOR: aufgabe_3 --->
Die binäre Kreuzentropie lässt sich mit Hilfe der Softmax-Funktion auch auf mehrere Klassen 
erweitern (multi-class classification). Die Softmax-Funktion 

$$
    \text{softmax}(\vec{z})_j = \frac{\exp(z_j)}{\sum_{k=1}^K \exp(z_k)}
$$

kann dabei als Aktivierungsfunktion der letzten Schicht eines neuronalen Netzes interpretiert werden, 
welche die Vorhersagen des Modells in eine normierte Wahrscheinlichkeitsverteilung umwandelt. 
Für den MNIST-Datensatz mit $K = 10$ Klassen (Ziffern von 0 bis 9) bedeutet das, 
dass für die Ausgabe des Modells $\hat{\vec{y}} = \hat{f}(\vec{x}) \in \mathbb{R}^{10}$ 
gilt, dass $\sum_{k=1}^{10} (\hat{\vec{y}})_k = 1$ und $(\hat{\vec{y}})_k \in [0, 1]$.

Die Softmax-Funktion wird in der Regel in Kombination mit der Kreuzentropie-Verlustfunktion 

$$
    \mathcal{H} = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K (\vec{y}_i)_k \log (\hat{\vec{y}}_i)_k
$$

verwendet, wobei $\vec{y}_i$ der One-Hot-Encoded Vektor des Labels des $i$-ten Datenpunkts ist.
Wir müssen beachten, dass sich dadurch auch die Ableitung der Verlustfunktion ändert.

**(a)**

Zeigen Sie, dass die Ableitung der Kreuzentropie-Verlustfunktion eines Datenpunktes
nach der Aktivierung der letzten Schicht $\vec{a}_L$ (vgl. {{eqref: eq:delta_L}}) gegeben ist durch

$$
    (\delta_L)_j = \frac{\partial \ell(\hat{\vec{y}}_i, \vec{y}_i)}{\partial (\vec{a}_L)_j} = \text{softmax}(\vec{a}_L)_j - (\vec{y}_i)_j.
$$

wobei wir annehmen, dass $\hat{\vec{y}}_i = \hat{f}(\vec{x}_i) = \sigma_L(\vec{a}_L) = \text{softmax}(\vec{a}_L)$.

**(b)**

Integrieren Sie die Softmax-Funktion und die Ableitung der Kreuzentropie-Verlustfunktion in Ihre 
Implementierung des MLPs und trainieren Sie das Modell auf dem MNIST-Datensatz. 

