# Übung 6

## Aufgabe 1: Gradienten der logistischen Regression

<!--- ANCHOR: aufgabe_1 --->
Zeigen Sie, dass die Gradienten der Verlustfunktion der logistischen Regression {{eqref: eq:logistic_loss}} 
nach den Parametern $\vec{w}$ und $b$ gegeben sind durch

$$
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \vec{w}} &= - \sum_{i=1}^N \left[ y_i - \hat{f}(\vec{x}_i) \right] \vec{x}_i \\
    \frac{\partial \mathcal{L}}{\partial b} &= - \sum_{i=1}^N \left[ y_i - \hat{f}(\vec{x}_i) \right] .
\end{align}
$$

Zeigen Sie dazu (unter Benutzung der Kettenregel) zunächst, dass die Ableitung der Sigmoid-Funktion 
$\sigma(z) = \frac{1}{1 + \exp(-z)}$ gegeben ist durch 

$$
    \frac{d \sigma(z)}{dz} = \sigma(z) (1 - \sigma(z)).
$$

Vergleichen Sie die Gradienten der logistischen Regression mit den Gradienten der linearen Regression 
für die Verlustfunktion der Methode der kleinsten Quadrate. Was fällt Ihnen auf?
<!--- ANCHOR_END: aufgabe_1 --->

## Aufgabe 2: Binäre Kreuzentropie

<!--- ANCHOR: aufgabe_2 --->
Wie wir schon in der Vorlesung gesehen haben, ist die Verlustfunktion der Methode der kleinsten 
Quadrate nicht die beste Methode, um Klassifikationsprobleme zu lösen. Für binäre 
Klassifikationsprobleme wird daher in der Regel die binäre Kreuzentropie 

$$
    \mathcal{H} = -\frac{1}{N} \sum_{i=1}^N y_i \log \hat{f}(\vec{x}_i) + (1 - y_i) \log(1 - \hat{f}(\vec{x}_i))
$$

als Verlustfunktion verwendet. Dabei ist $y_i \in \{0, 1\}$ das Label des $i$-ten Datenpunkts, 
und $\hat{f}(\vec{x}_i)$ die Vorhersage des Modells. Hier setzt die Verwendung des Logarithmus 
voraus, dass die Vorhersage des Modells in das Intervall $(0, 1)$ abgebildet wird, also 
Wahrhscheinlichkeiten repräsentiert, was durch die Sigmoid-Funktion erreicht wird.

Modifizieren Sie die Implementierung des SLP anhand des *Circles*-Datensatzes, sodass die 
binäre Kreuzentropie als Verlustfunktion verwendet wird. Beachten Sie, dass Sie dazu die 
Gradienten der Verlustfunktion $\mathcal{H}$ nach den Parametern $\vec{w}$ und $b$ berechnen 
müssen, wobei Ihnen die Ergebnisse der vorherigen Aufgabe helfen können.

Wie ist die Verlustfunktion der negativen log likelihood {{eqref: eq:logistic_loss}} 
der logistischen Regression mit der (binären) Kreuzentropie verwandt?
<!--- ANCHOR_END: aufgabe_2 --->

## Aufgabe 3: Softmax und Kreuzentropie

<!--- ANCHOR: aufgabe_3 --->
Die binäre Kreuzentropie lässt sich auf den Fall von mehreren Klassen erweitern 
(multi-class classification) durch die Verwendung der Softmax-Funktion. Die Softmax-Funktion 

$$
    \text{softmax}(\vec{z})_j = \frac{\exp(z_j)}{\sum_{k=1}^K \exp(z_k)}
$$

kann dabei als Aktivierungsfunktion der letzten Schicht eines neuronalen Netzes interpretiert werden, 
die die Vorhersagen des Modells in Wahrscheinlichkeiten umzuwandelt. Für den MNIST-Datensatz mit 
$K = 10$ Klassen (Ziffern von 0 bis 9) bedeutet das, dass die Ausgabe des Modells 
$\hat{f}(\vec{x}_i) \in \mathbb{R}^{10}$ durch die Softmax-Funktion in Wahrscheinlichkeiten 
$\hat{f}(\vec{x}_i) \in [0, 1]^{10}$ umgewandelt wird.

Die Softmax-Funktion wird in der Regel in Kombination mit der Kreuzentropie-Verlustfunktion 

$$
    \mathcal{H} = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K (\vec{y}_i)_k \log (\hat{f}(\vec{x}_i))_k
$$

verwendet, wobei $\vec{y}_i$ der One-Hot-Encoded Vektor des Labels des $i$-ten Datenpunkts ist.
Wir müssen beachten, dass sich dadurch auch die Ableitung der Verlustfunktion ändert.

**(a)**

Zeigen Sie, dass die Ableitung der Kreuzentropie-Verlustfunktion eines Datenpunktes
nach der Aktivierung der letzten Schicht $\vec{a}_L$ (vgl. {{eqref: eq:delta_L}}) gegeben ist durch

$$
    (\delta_L)_j = \frac{\partial \ell(\hat{f}(\vec{x}_i), \vec{y}_i)}{\partial (\vec{a}_L)_j} = \text{softmax}(\vec{a}_L)_j - (\vec{y}_i)_j.
$$

wobei wir annehmen, dass $\hat_{f}(\vec{x}_i) = \simga_L(\vec{a}_L) = \text{softmax}(\vec{a}_L)$.

**(b)**

Integrieren Sie die Softmax-Funktion und die Ableitung der Kreuzentropie-Verlustfunktion in Ihre 
Implementierung des MLPs und trainieren Sie das Modell auf dem MNIST-Datensatz. 

