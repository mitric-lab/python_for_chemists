# Problem Set 1

## Problem 1 [Homework]: Linear and Quadratic Regression

<!--- ANCHOR: aufgabe_1 --->
For the example in the lecture, we used the numpy function `np.linalg.solve` to solve the linear regression equation system *numerically*. In this context, this means that the computer follows a series of calculations and algorithms to find a generally *approximate* solution of the equation system. For the linear regression equation system, however, there is also an *analytical* solution that can be calculated using pen and paper.

**(a) Derivation of the analytical solution of linear regression**

Show that the solution of the equation system in matrix form
$$
  \underbrace{
  \begin{pmatrix}
    \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i \\[1.5em]
    \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2
  \end{pmatrix}}_{\displaystyle \bm{A}}
  \,
  \underbrace{
  \begin{pmatrix}
    \displaystyle \beta_0 \\[1.5em]
    \displaystyle \beta_1
  \end{pmatrix}
  \vphantom{
    \begin{pmatrix}
      \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
      \displaystyle \sum_{i=1}^N\, x_i y_i
    \end{pmatrix}
  }
  }_{\displaystyle \vec{x}}
  =
  \underbrace{
  \begin{pmatrix}
    \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
    \displaystyle \sum_{i=1}^N\, x_i y_i
  \end{pmatrix}}_ {\displaystyle \vec{b}} \,.
$$ 
is given by
$$
  \begin{align}
    \beta_0 &= \bar{y} - \beta_1 \bar{x} \\
    \beta_1 &= \frac{\sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{N} (x_i - \bar{x})^2}\,.
  \end{align}
$$

First solve the first equation in the system for $\beta_0$ and insert the result into the second equation. Use the definitions of the means $\bar{x}$ and $\bar{y}$.

<!-- **Solution:**

We start with the matrix equation from above. This corresponds to the following system of linear equations:
$$
  \begin{align}
    N \beta_0 + \left( \sum_{i=1}^N x_i \right) \beta_1 &= \sum_{i=1}^N y_i \\
    \left( \sum_{i=1}^N x_i \right) \beta_0 + \left( \sum_{i=1}^N x_i^2 \right) \beta_1 &= \sum_{i=1}^N x_i y_i 
  \end{align}
$$
First, solve the first equation for $\beta_0$:
$$
  N \beta_0 = \sum_{i=1}^N y_i - \left( \sum_{i=1}^N x_i \right) \beta_1
$$
Divide by $N$:
$$
  \beta_0 = \frac{\sum_{i=1}^N y_i}{N} - \frac{\sum_{i=1}^N x_i}{N} \beta_1
$$
Using the definitions of the means $\bar{y} = \frac{1}{N} \sum_{i=1}^N y_i$ and $\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i$, we get:
$$
  \beta_0 = \bar{y} - \bar{x} \beta_1
$$
This is the first part of the desired solution. 

```admonish note title="Note"
  By rearranging the terms to $\bar{y} = \beta_1 \bar{x} + \beta_0$, we can see that the point defined by the means, $(\bar{x}, \bar{y})$, satisfies the linear regression equation. Therefore, the regression line always passes through this point.
```

Now, we solve the second equation for $\beta_1$. Simplifying the equation, we get:
$$
  \begin{aligned}
    & \sum_i x_i \beta_0+\sum_i x_i^2 \beta_1=\sum_i x_i y_i \\
    & \sum_1 x_1 y_i-\sum_i x_i \beta_0-\sum_i x_1^2 \beta_i=0 \\
    & \sum_i x_i\left(y_i-\beta_0-x_i \beta_1\right)=0 \\
  \end{aligned}
$$
We can then insert the solution for $\beta_0$:
$$
  \begin{aligned}
    & \sum_i x_i\left(y_i-\bar{y}+\beta_1 \bar{x}-x_i \beta_1\right)=0 \\
    & \sum_i x_i\left(y_i-\bar{y}+\beta_1\left(\bar{x}-x_i\right)\right)=0 \\
  \end{aligned}
$$
After distributing the sum, we get:
$$
  \begin{aligned}
    & \sum_i x_i\left(y_i-\bar{y}\right)+\beta_1 \sum_i x_i\left(\bar{x}-x_i\right)=0 \\
    & \sum_i x_i\left(y_i-\bar{y}\right)=\beta_1 \sum_i x_i\left(x_i-\bar{x}\right) \\
    & \beta_1=\frac{\sum_i x_i\left(y_i-\bar{y}\right)}{\sum_i x_i\left(x_i-\bar{x}\right)} \\
  \end{aligned}
$$
This is already a perfectly valid solution. However, we can still simplify it by expanding the numerator and the denominator:
$$
  \begin{aligned}
    & \beta_1=\frac{\sum_i x_i\left(y_i-\bar{y}\right)}{\sum_i x_i\left(x_i-\bar{x}\right)} \\
    & =\frac{\sum_i x_i\left(y_i-\bar{y}\right)-\sum_i \bar{x}\left(y_i-\bar{y}\right)}{\sum_i x_i\left(x_i-\bar{x}\right)-\sum_i \bar{x}\left(x_i-\bar{x}\right)} \\
    & =\frac{\sum_i\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_i\left(x_i-\bar{x}\right)^2}
  \end{aligned}
$$
This is the second part of the desired solution.

```admonish note title="Note"
  The equation for $\beta_1$ is closely related to the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) $r$, which is a measure of the linear relationship between two variables. It is a standardized measure that ranges from $-1$ to $1$, where $-1$ indicates a perfect negative linear relationship, $0$ indicates no linear relationship, and $1$ indicates a perfect positive linear relationship.
``` -->

**(b) Implementation of the analytical solution for methylene blue measurement data**

Use the analytical solution to explicitly calculate the linear regression parameters for the methylene blue measurement data from the lecture. Compare the results with the results obtained using `np.linalg.solve`.

<!-- 
**Solution:**
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_b}}
```
-->

**(c) Derivation of the matrix equation for quadratic regression**

The quadratic regression is an extension of linear regression, where the dependent variable $y$ is approximated by a polynomial of second degree in the independent variable $x$. The general form of the quadratic regression is given by
$$
  \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2
$$

In analogy to linear regression, the quadratic regression can be viewed as a linear model in the parameters $\beta = (\beta_0, \beta_1, \beta_2)$. Show that this model is described by the following matrix equation:
$$
    \begin{pmatrix}
        \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 & \displaystyle \sum_{i=1}^N\, x_i^3 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i ^2 & \displaystyle \sum_{i=1}^N\, x_i^3 & \displaystyle \sum_{i=1}^N\, x_i^4
    \end{pmatrix}
    \begin{pmatrix}
        \displaystyle \beta_0 \\[1.5em]
        \displaystyle \beta_1 \\[1.5em]
        \displaystyle \beta_2
    \end{pmatrix}
    \vphantom{
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    }
    =
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
$$

To do this, insert the quadratic function $\hat{f}(\beta; x_i)$ into the general form of the loss function of the least squares method and take the derivatives with respect to the target parameters.

<!-- **Solution:**

Simply follow the steps from the derivation of the linear regression equation system in [lecture 1.2.](../01-regression/02-linear_regression.md). -->

**(d) Implementation of quadratic regression for methylene blue measurement data**

Now proceed as for linear regression and numerically solve the equation system of quadratic regression from part (c) for the methylene blue data. Construct the required matrix and vector in the form of arrays first and use the function `np.linalg.solve`. Then plot the quadratic regression together with the data points. Also plot the residuals and compare the results with the linear regression.

<!-- 
**Solution:**
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_d}}
```
-->

<!--- ANCHOR_END: aufgabe_1 --->

## Problem 2: Polynomial Regression

<!--- ANCHOR: aufgabe_2 --->

Based on the previous problem, where you transitioned from linear regression to quadratic regression, you can already guess that the method of least squares is also simple to implement with higher-degree polynomials. In practice, however, it is not practical to solve the systems of equations for higher-degree polynomials manually. Instead, you can use the function [`np.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) to determine the coefficients ${\beta_0, \beta_1, \ldots, \beta_n}$ of a polynomial of degree $n$
$$
\begin{equation}
    \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \ldots + \beta_n x_i^n
\end{equation}
$$
that best fits the data. This function takes the arrays of the independent variable $x$ and the dependent variable $y$, as well as the degree of the polynomial $n$, as arguments and returns the coefficients $\beta_j$:

**(a) Polynomial Regression with `np.polyfit`**

Apply the `np.polyfit` function to the methylene blue data to fit a 20th-degree polynomial and plot the polynomial together with the data points.

```admonish tip title="Tip"
To plot the polynomial function, you can use the function [`np.polyval`](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html), which calculates the function values of the polynomial for given values of $\beta_j$ and $x$ (i.e. `concentrations`) in the form of an array.
```

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_a}}
```
-->

**(b) Prediction of New Data Points**

From the plot of the polynomial regression (and possibly the residuals), you can see that the 20th-degree polynomial fits the data points very well. This is also not surprising, as we can fit a function with at least 20 parameters so that it perfectly reproduces our 20 data points. However, in our code so far, we have only considered the data points used to fit the model for visualizing the results. The function values between the data points were merely interpolated. Typically, however, we also want to use our model to obtain predictions for new data points that lie within or outside the range of the data points.

Plot the entire 20th-degree polynomial together with the data points in the interval $0 \leq x \leq 50.0$. To do this, define an array with 1000 values using the function [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) and calculate the function values of the polynomial for these values. Limit the plot display to the range $0 \leq y \leq 2.0$. What do you observe?

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_b}}
```
-->

<!--- ANCHOR_END: aufgabe_2 --->

## Problem 3: Regularization

<!--- ANCHOR: aufgabe_3 --->

The phenomenon that you can observe with the 20th-degree polynomial regression is known as *overfitting*. It occurs when a model becomes excessively complex -- too many parameters -- allowing it to capture not just the meaningful patterns in the data, but also the random noise and fluctuations. While such a model may achieve near-perfect accuracy on the training data points, it performs poorly when making predictions on new, unseen data.

One approach to combat overfitting is through *regularization* - a technique that constrains the model's complexity by adding penalties to the optimization process. A common regularization strategy involves keeping the model coefficients $\beta_j$ small by incorporating an additional term in the loss function. This helps prevent the model from becoming too sensitive to individual data points and promotes smoother, more generalizable solutions.

If the square of the $\ell_2$-norm of the coefficients $\| \beta \|^2 = \sum_i \beta_i^2$ is used as regularization and added to the loss function, it is called *Ridge Regression*. The loss function is then given by
$$
\begin{equation}
    L(\beta; x, y) = \sum_{i=1}^{N} (y_i - \hat{f}(\beta; x_i))^2 + \lambda \| \beta \|^2,
\end{equation}
$$
where the parameter $\lambda$ determines the relative strength of the regularization. Adding the term $\lambda \| \beta \|^2$ penalizes large coefficient values, discouraging the model from fitting the noise in the data too closely and thus promoting a smoother, potentially more generalizable fit.

**(a) Ridge Regression of the Methylene Blue Data with a 20th-Order Polynomial**

Implement Ridge Regression for the methylene blue data with $\lambda = 0.001$ and fit a 20th-degree polynomial. Use the numerical optimization method with the function `minimize` and modify your objective function accordingly. Use an array of zeros as starting values. Also, normalize the concentrations and absorbance values to the range $[0, 1]$ before the regression by dividing each by its maximum value. Plot the result together with the data points.

```admonish tip title="Tip"
Use the `np.polyval` function again to define the loss function, as well as the function `np.linalg.norm` to calculate the $\ell_2$-norm of the coefficients. Don't forget to introduce the parameter $\lambda$ into the loss function and pass it to the `minimize` function.
```

<!--
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_03_a}}
```
-->

**(b) Influence of the Regularization Parameter $\lambda$**

Vary the regularization parameter $\lambda$ and observe how the strength of the regularization affects the model's fit to the data points. What happens if you choose $\lambda = 0$ or $\lambda = 1$?

<!--- ANCHOR_END: aufgabe_3 --->

