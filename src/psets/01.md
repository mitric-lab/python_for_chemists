# Übung 1


## Aufgabe 1: Lineare und quadratische Regression

<!--- ANCHOR: aufgabe_1 --->

In dem obigen Beispiel haben wir die numpy Funktion `np.linalg.solve` verwendet, um die Lösung des 
Gleichungssystems der linearen Regression *numerisch* zu berechnen. In diesem Zusammenhang bedeutet 
dies, dass der Computer einer Reihe von Rechenschritten und Algorithmen folgt, um die approximative 
Lösung des Gleichungssystems zu finden. Für das Gleichungssystem der linearen Regression gibt es jedoch 
auch eine *analytische* Lösung, die direkt berechnet werden kann.

**(a) Analytische Lösung der linearen Regression herleiten**

Zeigen Sie, dass die Lösung des Gleichungssystems {{eqref: eq:least_squares_linear_params}} 
gegeben ist durch:

$$
    \begin{align}
        \beta_0 &= \bar{y} - \beta_1 \bar{x} \\
        \beta_1 &= \frac{\sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{N} (x_i - \bar{x})^2}
    \end{align}
$$

Lösen Sie dazu zunächst die erste Gleichung in {{eqref: eq:least_squares_linear_params}} nach $\beta_0$ 
auf und setzen Sie das Ergebnis in die zweite Gleichung ein. Verwenden Sie außerdem die Definitionen der 
Mittelwerte $\bar{x}$ und $\bar{y}$.

**(b) Implementieren der analytischen Lösung für Messdaten von Methylenblau**

Nutzen Sie die analytische Lösung, um die Parameter der linearen Regression für die Messdaten von Methylenblau 
explizit zu berechnen. Vergleichen Sie die Ergebnisse mit den Ergebnissen, die Sie mit `np.linalg.solve` 
erhalten haben.

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_b}}
```
-->

**(c) Matrixgleichung der quadratischen Regression herleiten**

Die quadratische Regression ist eine Erweiterung der linearen Regression, bei der die abhängige Variable $y$ 
durch ein Polynom zweiten Grades in der unabhängigen Variable $x$ angenähert wird. Die allgemeine Form der 
quadratischen Regression ist gegeben durch:

$$
  \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2
  {{numeq}}{eq:quad_model}
$$

In Analogie zur linearen Regression können wir die quadratische Regression als ein lineares Modell in 
den Parametern $\beta = (\beta_0, \beta_1, \beta_2)$ auffassen. Zeigen Sie, dass dieses Modell durch 
die folgende Matrixgleichung beschrieben wird:

$$
    \begin{pmatrix}
        \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 & \displaystyle \sum_{i=1}^N\, x_i^3 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i ^2 & \displaystyle \sum_{i=1}^N\, x_i^3 & \displaystyle \sum_{i=1}^N\, x_i^4
    \end{pmatrix}
    \begin{pmatrix}
        \displaystyle \beta_0 \\[1.5em]
        \displaystyle \beta_1 \\[1.5em]
        \displaystyle \beta_2
    \end{pmatrix}
    \vphantom{
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    }
    =
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    {{numeq}}{eq:least_squares_quad_params}
$$

Setzen Sie dazu die quadratische Funktion $\hat{f}(\beta; x_i)$ in die allgemeine Form der Methode 
der kleinsten Quadrate {{eqref: eq:least_squares_loss}} ein und bilden Sie die Ableitungen 
nach den gesuchten Parametern.

**(d) Quadratische Regression implementieren und an Methylenblau-Daten 
anwenden**

Fahren Sie nun fort wie für die lineare Regression, indem Sie das Gleichungssystem der quadratischen 
Regression {{eqref: eq:least_squares_quad_params}} für die Methylenblau-Daten numerisch lösen. 
Konstruieren Sie dazu zunächst die benötigte Matrix, bzw. den Vektor in Form von Arrays, und verwenden 
Sie die Funktion `np.linalg.solve`. Plotten Sie anschließend die quadratische Regression zusammen 
mit den Datenpunkten. Plotten Sie ebenfalls die Resiuduen und vergleichen Sie die Ergebnisse mit der 
linearen Regression.

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_d}}
```
-->

<!--- ANCHOR_END: aufgabe_1 --->

## Aufgabe 2: Polynomiale Regression

<!--- ANCHOR: aufgabe_2 --->

Basierend auf der vorherigen Aufgabe, in welcher Sie von der linearen
Regression zur quadratischen Regression übergegangen sind, können Sie
bereits vermuten, dass die Methode der kleinsten Quadrate auch mit
höhergradige Polynomen simpel zu implementieren ist. In der Praxis ist es jedoch
nicht sinnvoll, die Gleichungssysteme für Polynome höheren Grades
manuell zu lösen. Stattdessen können Sie die Funktion 
[`np.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)
verwenden, um die Koeffizienten ${\beta_0, \beta_1, \ldots, \beta_n}$ eines Polynoms $n$-ten Grades
$$
\begin{equation}
    \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \ldots + \beta_n x_i^n
    {{numeq}}{eq:poly_model}
\end{equation}
$$
zu bestimmen, welches am besten zu den Daten passt. Diese Funktion
nimmt als Argumente die Arrays der unabhängigen Variable $x$ und 
der abhängigen Variable $y$, sowie den Grad des Polynoms $n$ entgegen und
gibt die Koeffizienten $\beta_j$ zurück.

**(a) Polynomiale Regression mit `np.polyfit`**

Wenden Sie die Funktion `np.polyfit` auf die Methylenblau-Daten an, um
ein Polynom 20. Grades zu fitten und plotten Sie das Polynom zusammen mit den Datenpunkten.

```admonish tip title="Tipp"
Zum Plotten der Polynomfunktion können Sie die Funktion 
[`np.polyval`](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html) verwenden, welche
die Funktionswerte des Polynoms für gegebene Werte von $\beta_j$ und $x$ (d.h. `concentrations`) in 
Form eines Arrays berechnet.
```

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_a}}
```
-->

**(b) Vorhersage von neuen Datenpunkten**

Aus dem Plot der polynomialen Regression (und ggf. den Residuen) können Sie erkennen, 
dass das Polynom 20. Grades die Datenpunkte sehr gut anpasst. Dies ist auch nicht weiter
verwunderlich, da wir eine Funktion mit mind. 20 Parametern so anpassen können, dass sie 
unsere 20 Datenpunkte perfekt wiedergibt. Allerdings haben wir in unserem
Code bisher lediglich die Datenpunkte, welche wir zum Fitten des Modells verwendet haben, zur 
Visualisierung der Ergebnisse beachtet. Die Funktionswerte zwischen den Datenpunkten wurden 
lediglich interpoliert. In der Regel möchten wir allerdings mit Hilfe unseres Modells auch 
Vorhersagen für neue Datenpunkte erhalten. 

Plotten Sie das gesamte Polynom 20. Grades zusammen mit den Datenpunkten in dem Interall
$0 \leq x \leq 50.0$. Definieren Sie sich dazu ein Array mit 1000 Werten mit Hilfe 
der Funktion [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) 
und berechnen Sie die Funktionswerte. Beschränken Sie die Darstellung des Plots auf den Bereich $0 \leq y \leq 2.0$.
Was beobachten Sie?

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_b}}
```
-->

<!--- ANCHOR_END: aufgabe_2 --->

## Aufgabe 3: Regularisierung

<!--- ANCHOR: aufgabe_3 --->

Das Phänomen, welches Sie bei der polynomialen Regression 20. Ordnung beobachten können,
wird als *Überanpassung* oder *overfitting* bezeichnet. Es tritt auf, wenn das Modell zu komplex ist
und nicht nur der zugrunde liegende Trend, sondern auch das Rauschen in den Daten
angepasst wird. In solchen Fällen kann das Modell die Datenpunkte zwar perfekt reproduzieren, 
aber es wird nicht in der Lage sein, neue Datenpunkte vorherzusagen.

Um Überanpassung zu vermeiden gibt es, neben der Reduzierung der Parameter, die Möglichkeit der 
*Regularisierung*. Darunter versteht man die Einführung von zusätzlichen Bedingungen, welche die 
Komplexität des Modells einschränken. Eine solche Bedingung kann beispielsweise sein, dass die
Koeffizienten $\beta_j$ möglichst klein gehalten werden, was durch die Einführung eines
zusätzlichen Terms in die Kostenfunktion erreicht werden kann. 

Verwendet man das Quadrat der $\ell_2$-Norm der Koeffizienten $\| \beta \|^2 = \sum_i \beta_i^2$ 
als Regularisierung und fügt sie der Verlustfunktion hinzu, so spricht man von *Ridge-Regression*. 
Die Verlustfunktion ist dann gegeben durch
$$
\begin{equation}
    L(\beta; x, y) = \sum_{i=1}^{N} (y_i - \hat{f}(\beta; x_i))^2 + \lambda \| \beta \|^2,
    {{numeq}}{eq:ridge_loss}
\end{equation}
$$
wobei der Parameter $\lambda$ die relative Stärke der Regularisierung bestimmt.

**(a) Ridge-Regression der Methylenblau-Daten mit Polynom 20. Ordnung**

Implementieren Sie die Ridge-Regression für die Methylenblau-Daten mit $\lambda = 0.001$
und fitten Sie ein Polynom 20. Ordnung. Nutzen Sie dazu die numerische Optimierungsmethode
mit der Funktion `minimize` und ändern Sie Ihre Objektivfunktion entsprechend. Verwenden Sie
als Startwerte ein Array mit Nullen. Normalisieren Sie außerdem vor der Regression die 
Konzentrationen und die Absorptionswerte auf den Bereich $[0, 1]$, indem Sie jeweils durch den Maximalwert 
teilen. Plotten Sie das Ergebnis zusammen mit den Datenpunkten.

```admonish tip title="Tipp"
Nutzen Sie zur Definition der Verlustfunktion erneut die Funktion `np.polyval`, sowie
die Funktion `np.linalg.norm` zur Berechnung der $\ell_2$-Norm der Koeffizienten. Vergessen Sie nicht, den
Parameter $\lambda$ in die Verlustfunktion einzuführen und der Funktion `minimize` zu übergeben.
```

<!--
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_03_a}}
```
-->

**(b) Einfluss des Regularisierungsparameters $\lambda$**

Variieren Sie den Regularisierungsparameter $\lambda$ und beobachten Sie, wie sich die Stärke der
Regularisierung auf die Anpassung des Modells an die Datenpunkte auswirkt. Was passiert, wenn Sie $\lambda = 0$ oder 
$\lambda = 1$ wählen?

<!--- ANCHOR_END: aufgabe_3 --->

