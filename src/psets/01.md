# Problem Set 1

## Problem 1: Linear and Quadratic Regression

<!--- ANCHOR: aufgabe_1 --->
For the example in the lecture, we used the numpy function `np.linalg.solve` 
to solve the linear regression equation system *numerically*. In this context,
this means that the computer follows a series of calculations and algorithms
to find an in general *approximate* solution of the equation system. For the 
linear regression equation system, however, there is also an *analytical* 
solution that can be calculated directly.

**(a) Derivation of the analytical solution of linear regression**

Show that the solution of the equation system in matrix form
$$
  \underbrace{
  \begin{pmatrix}
    \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i \\[1.5em]
    \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2
  \end{pmatrix}}_{\displaystyle \bm{A}}
  \,
  \underbrace{
  \begin{pmatrix}
    \displaystyle \beta_0 \\[1.5em]
    \displaystyle \beta_1
  \end{pmatrix}
  \vphantom{
    \begin{pmatrix}
      \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
      \displaystyle \sum_{i=1}^N\, x_i y_i
    \end{pmatrix}
  }
  }_{\displaystyle \vec{x}}
  =
  \underbrace{
  \begin{pmatrix}
    \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
    \displaystyle \sum_{i=1}^N\, x_i y_i
  \end{pmatrix}}_ {\displaystyle \vec{b}} \,.
$$ 
is given by
$$
    \begin{align}
        \beta_0 &= \bar{y} - \beta_1 \bar{x} \\
        \beta_1 &= \frac{\sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{N} (x_i - \bar{x})^2}\,.
    \end{align}
$$

First solve the first equation in the system for $\beta_0$ and insert 
the result into the second equation. 
Use the definitions of the means $\bar{x}$ and $\bar{y}$.

**(b) Implementation of the analytical solution for methylene blue measurement data**

Use the analytical solution to explicitly calculate the linear regression 
parameters for the methylene blue measurement data.
Compare the results with the results obtained using `np.linalg.solve`.


<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_b}}
```
-->

**(c) Derivation of the matrix equation for quadratic regression**

The quadratic regression is an extension of linear regression, where the 
dependent variable $y$ is approximated by a polynomial of second degree 
in the independent variable $x$. The general form of the quadratic regression 
is given by
$$
  \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2
$$

In analogy to linear regression, the quadratic regression can be viewed as 
a linear model in the parameters $\beta = (\beta_0, \beta_1, \beta_2)$.
Show that this model is described by the following matrix equation:
$$
    \begin{pmatrix}
        \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 & \displaystyle \sum_{i=1}^N\, x_i^3 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i ^2 & \displaystyle \sum_{i=1}^N\, x_i^3 & \displaystyle \sum_{i=1}^N\, x_i^4
    \end{pmatrix}
    \begin{pmatrix}
        \displaystyle \beta_0 \\[1.5em]
        \displaystyle \beta_1 \\[1.5em]
        \displaystyle \beta_2
    \end{pmatrix}
    \vphantom{
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    }
    =
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
$$

To do this, insert the quadratic function $\hat{f}(\beta; x_i)$ into the 
general form of the loss function of the least squares method and take the 
derivatives with respect to the parameters sought.

**(d) Implementation of quadratic regression for methylene blue measurement data**

Now proceed as for linear regression and numerically solve the equation system
of quadratic regression from part (c) for the methylene blue data.
Construct the required matrix and vector in the form of arrays first and
use the function `np.linalg.solve`. Then plot the quadratic regression
together with the data points. Also plot the residuals and compare the results
with the linear regression.

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_d}}
```
-->

<!--- ANCHOR_END: aufgabe_1 --->

## Aufgabe 2: Polynomiale Regression

<!--- ANCHOR: aufgabe_2 --->

Basierend auf der vorherigen Aufgabe, in welcher Sie von der linearen
Regression zur quadratischen Regression übergegangen sind, können Sie
bereits vermuten, dass die Methode der kleinsten Quadrate auch mit
höhergradige Polynomen simpel zu implementieren ist. In der Praxis ist es jedoch
nicht sinnvoll, die Gleichungssysteme für Polynome höheren Grades
manuell zu lösen. Stattdessen können Sie die Funktion 
[`np.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)
verwenden, um die Koeffizienten ${\beta_0, \beta_1, \ldots, \beta_n}$ eines Polynoms $n$-ten Grades
$$
\begin{equation}
    \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \ldots + \beta_n x_i^n
\end{equation}
$$
zu bestimmen, welches am besten zu den Daten passt. Diese Funktion
nimmt als Argumente die Arrays der unabhängigen Variable $x$ und 
der abhängigen Variable $y$, sowie den Grad des Polynoms $n$ entgegen und
gibt die Koeffizienten $\beta_j$ zurück.

**(a) Polynomiale Regression mit `np.polyfit`**

Wenden Sie die Funktion `np.polyfit` auf die Methylenblau-Daten an, um
ein Polynom 20. Grades zu fitten und plotten Sie das Polynom zusammen mit den Datenpunkten.

```admonish tip title="Tipp"
Zum Plotten der Polynomfunktion können Sie die Funktion 
[`np.polyval`](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html) verwenden, welche
die Funktionswerte des Polynoms für gegebene Werte von $\beta_j$ und $x$ (d.h. `concentrations`) in 
Form eines Arrays berechnet.
```

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_a}}
```
-->

**(b) Vorhersage von neuen Datenpunkten**

Aus dem Plot der polynomialen Regression (und ggf. den Residuen) können Sie erkennen, 
dass das Polynom 20. Grades die Datenpunkte sehr gut anpasst. Dies ist auch nicht weiter
verwunderlich, da wir eine Funktion mit mind. 20 Parametern so anpassen können, dass sie 
unsere 20 Datenpunkte perfekt wiedergibt. Allerdings haben wir in unserem
Code bisher lediglich die Datenpunkte, welche wir zum Fitten des Modells verwendet haben, zur 
Visualisierung der Ergebnisse beachtet. Die Funktionswerte zwischen den Datenpunkten wurden 
lediglich interpoliert. In der Regel möchten wir allerdings mit Hilfe unseres Modells auch 
Vorhersagen für neue Datenpunkte erhalten. 

Plotten Sie das gesamte Polynom 20. Grades zusammen mit den Datenpunkten in dem Interall
$0 \leq x \leq 50.0$. Definieren Sie sich dazu ein Array mit 1000 Werten mit Hilfe 
der Funktion [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) 
und berechnen Sie die Funktionswerte. Beschränken Sie die Darstellung des Plots auf den Bereich $0 \leq y \leq 2.0$.
Was beobachten Sie?

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_b}}
```
-->

<!--- ANCHOR_END: aufgabe_2 --->

## Aufgabe 3: Regularisierung

<!--- ANCHOR: aufgabe_3 --->

Das Phänomen, welches Sie bei der polynomialen Regression 20. Ordnung beobachten können,
wird als *Überanpassung* (engl. *overfitting*) bezeichnet. Es tritt auf, wenn das Modell zu komplex ist
und nicht nur der zugrunde liegende Trend, sondern auch das Rauschen in den Daten
angepasst wird. In solchen Fällen kann das Modell die Datenpunkte zwar perfekt reproduzieren, 
aber es wird nicht in der Lage sein, neue Datenpunkte vorherzusagen.

Um Überanpassung zu vermeiden gibt es, neben der Reduzierung der Parameter, die Möglichkeit der 
*Regularisierung*. Darunter versteht man die Einführung von zusätzlichen Bedingungen, welche die 
Komplexität des Modells einschränken. Eine solche Bedingung kann beispielsweise sein, dass die
Koeffizienten $\beta_j$ möglichst klein gehalten werden, was durch die Einführung eines
zusätzlichen Terms in die Verlustfunktion erreicht werden kann. 

Verwendet man das Quadrat der $\ell_2$-Norm der Koeffizienten $\| \beta \|^2 = \sum_i \beta_i^2$ 
als Regularisierung und fügt sie der Verlustfunktion hinzu, so spricht man von *Ridge-Regression*. 
Die Verlustfunktion ist dann gegeben durch
$$
\begin{equation}
    L(\beta; x, y) = \sum_{i=1}^{N} (y_i - \hat{f}(\beta; x_i))^2 + \lambda \| \beta \|^2,
\end{equation}
$$
wobei der Parameter $\lambda$ die relative Stärke der Regularisierung bestimmt.

**(a) Ridge-Regression der Methylenblau-Daten mit Polynom 20. Ordnung**

Implementieren Sie die Ridge-Regression für die Methylenblau-Daten mit $\lambda = 0.001$
und fitten Sie ein Polynom 20. Ordnung. Nutzen Sie dazu die numerische Optimierungsmethode
mit der Funktion `minimize` und ändern Sie Ihre Objektivfunktion entsprechend. Verwenden Sie
als Startwerte ein Array mit Nullen. Normalisieren Sie außerdem vor der Regression die 
Konzentrationen und die Absorptionswerte auf den Bereich $[0, 1]$, indem Sie jeweils durch den Maximalwert 
teilen. Plotten Sie das Ergebnis zusammen mit den Datenpunkten.

```admonish tip title="Tipp"
Nutzen Sie zur Definition der Verlustfunktion erneut die Funktion `np.polyval`, sowie
die Funktion `np.linalg.norm` zur Berechnung der $\ell_2$-Norm der Koeffizienten. Vergessen Sie nicht, den
Parameter $\lambda$ in die Verlustfunktion einzuführen und der Funktion `minimize` zu übergeben.
```

<!--
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_03_a}}
```
-->

**(b) Einfluss des Regularisierungsparameters $\lambda$**

Variieren Sie den Regularisierungsparameter $\lambda$ und beobachten Sie, wie sich die Stärke der
Regularisierung auf die Anpassung des Modells an die Datenpunkte auswirkt. Was passiert, wenn Sie $\lambda = 0$ oder 
$\lambda = 1$ wählen?

<!--- ANCHOR_END: aufgabe_3 --->

