# Übung 1


## Aufgabe 1: Lineare und quadratische Regression

<!--- ANCHOR: aufgabe_1 --->

**(a) Analytische Lösung der linearen Regression herleiten**

In dem obigen Beispiel haben wir die numpy Funktion `np.linalg.solve` verwendet, um die Lösung des 
Gleichungssystems der linearen Regression *numerisch* zu berechnen. In diesem Zusammenhang bedeutet 
dies, dass der Computer einer Reihe von Rechenschritten und Algorithmen folgt, um die approximative 
Lösung des Gleichungssystems zu finden. Für das Gleichungssystem der linearen Regression gibt es jedoch 
auch eine *analytische* Lösung, die direkt berechnet werden kann.

Zeigen Sie, dass die Lösung des Gleichungssystems {{eqref: eq:least_squares_linear_params}} 
gegeben ist durch:

$$
    \begin{align}
        \beta_0 &= \bar{y} - \beta_1 \bar{x} \\
        \beta_1 &= \frac{\sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{N} (x_i - \bar{x})^2}
    \end{align}
$$

Lösen Sie dazu zunächst die erste Gleichung in {{eqref: eq:least_squares_linear_params}} nach $\beta_0$ 
auf und setzen Sie das Ergebnis in die zweite Gleichung ein. Verwenden Sie außerdem die Definitionen der 
Mittelwerte $\bar{x}$ und $\bar{y}$.

**(b) Implementieren der analytischen Lösung für Messdaten von Methylenblau**

Nutzen Sie die analytische Lösung, um die Parameter der linearen Regression für die Messdaten von Methylenblau 
explizit zu berechnen. Vergleichen Sie die Ergebnisse mit den Ergebnissen, die Sie mit `np.linalg.solve` 
erhalten haben.

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_b}}
```
-->

**(c) Matrixgleichung der quadratischen Regression herleiten**

Die quadratische Regression ist eine Erweiterung der linearen Regression, bei der die abhängige Variable $y$ 
durch ein Polynom zweiten Grades in der unabhängigen Variable $x$ angenähert wird. Die allgemeine Form der 
quadratischen Regression ist gegeben durch:

$$
  \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2
  {{numeq}}{eq:quad_model}
$$

In Analogie zur linearen Regression können wir die quadratische Regression als ein lineares Modell in 
den Parametern $\beta = (\beta_0, \beta_1, \beta_2)$ auffassen. Zeigen Sie, dass dieses Modell durch 
die folgende Matrixgleichung beschrieben wird:

$$
    \begin{pmatrix}
        \displaystyle N & \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i & \displaystyle \sum_{i=1}^N\, x_i^2 & \displaystyle \sum_{i=1}^N\, x_i^3 \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i ^2 & \displaystyle \sum_{i=1}^N\, x_i^3 & \displaystyle \sum_{i=1}^N\, x_i^4
    \end{pmatrix}
    \begin{pmatrix}
        \displaystyle \beta_0 \\[1.5em]
        \displaystyle \beta_1 \\[1.5em]
        \displaystyle \beta_2
    \end{pmatrix}
    \vphantom{
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    }
    =
    \begin{pmatrix}
        \displaystyle \sum_{i=1}^N\, y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i y_i \\[1.5em]
        \displaystyle \sum_{i=1}^N\, x_i^2 y_i
    \end{pmatrix}
    {{numeq}}{eq:least_squares_quad_params}
$$

Setzen Sie dazu die quadratische Funktion $\hat{f}(\beta; x_i)$ in die allgemeine Form der Methode 
der kleinsten Quadrate {{eqref: eq:least_squares_linear_params}} ein und bilden Sie die Ableitungen 
nach den gesuchten Parametern.

**(d) Quadratische Regression implementieren und an Methylenblau-Daten 
anwenden**

Fahren Sie nun fort wie für die lineare Regression, indem Sie das Gleichungssystem der quadratischen 
Regression {{eqref: eq:least_squares_quad_params}} für die Methylenblau-Daten numerisch lösen. 
Konstruieren Sie dazu zunächst die benötigte Matrix, bzw. den Vektor in Form von Arrays, und verwenden 
Sie die Funktion `np.linalg.solve`. Plotten Sie anschließend die quadratische Regression zusammen 
mit den Datenpunkten. Plotten Sie ebenfalls die Resiuduen und vergleichen Sie die Ergebnisse mit der 
linearen Regression.

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_01_d}}
```
-->

<!--- ANCHOR_END: aufgabe_1 --->

## Aufgabe 2: Polynomiale Regression

<!--- ANCHOR: aufgabe_2 --->

**(a) Polynomiale Regression mit `np.polyfit`**

Basierend auf der vorherigen Aufgabe, in welcher Sie von der linearen
Regression zur quadratischen Regression übergegangen sind, können Sie
bereits vermuten, dass die Methode der kleinsten Quadrate auch mit
höhergradige Polynomen simpel zu implementieren ist. In der Praxis ist es jedoch
nicht sinnvoll, die Gleichungssysteme für Polynome höheren Grades
manuell zu lösen. Stattdessen können Sie die Funktion 
[`np.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)
verwenden, um die Koeffizienten ${\beta_0, \beta_1, \ldots, \beta_n}$ eines Polynoms $n$-ten Grades
$$
\begin{equation}
    \hat{f}(\beta; x_i) = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \ldots + \beta_n x_i^n
    {{numeq}}{eq:poly_model}
\end{equation}
$$
zu bestimmen, welches am besten zu den Daten passt. Diese Funktion
nimmt als Argumente die Arrays der unabhängigen Variable $x$ und 
der abhängigen Variable $y$, sowie den Grad des Polynoms $n$ entgegen und
gibt die Koeffizienten $\beta_j$ zurück.

Wenden Sie die Funktion `np.polyfit` auf die Methylenblau-Daten an, um
ein Polynom 20. Grades zu fitten und plotten Sie das Polynom zusammen mit den Datenpunkten.

```admonish tip title="Tipp"
Zum Plotten der Polynomfunktion können Sie die Funktion 
[`np.polyval`](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html) verwenden, welche
die Funktionswerte des Polynoms für gegebene Werte von $\beta_j$ und $x$ (d.h. `concentrations`) in 
Form eines Arrays berechnet.
```

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_a}}
```
-->

**(b) Vorhersage von neuen Datenpunkten***

Aus dem Plot der polynomialen Regression (und ggf. den Residuen) können Sie erkennen, 
dass das Polynom 20. Grades die Datenpunkte sehr gut anpasst. Allerdings haben wir in unserem
Code bisher lediglich die Datenpunkte, welche wir zum Fitten des Modells verwendet haben, zur 
Visualisierung der Ergebnisse beachtet. Die Funktionswerte zwischen den Datenpunkten wurden 
lediglich interpoliert. In der Regel möchten wir allerdings mit Hilfe unseres Modells auch Vorhersagen für neue 
Datenpunkte erhalten. 

Plotten Sie das gesamte Polynom 20. Grades zusammen mit den Datenpunkten in dem Interall
$0 \leq x \leq 50.0$. Definieren Sie sich dazu ein Array mit 1000 Werten mit Hilfe 
der Funktion [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) 
und berechnen Sie die Funktionswerte. Beschränken Sie die Darstellung des Plots auf den Bereich $0 \leq y \leq 2.0$.
Was beobachten Sie?

<!-- 
Lösung:
```python
{{include ../codes/01-regression/exercise_01.py:exercise_02_b}}
```
-->

<!--- ANCHOR_END: aufgabe_2 --->

Dieses Phänomen wird als *Überanpassung* oder *Überanpassung* bezeichnet. Es tritt auf, wenn das 
Modell zu komplex ist und die Datenpunkte nicht nur durch den zugrunde liegenden Trend, sondern 
auch durch das Rauschen in den Daten beschrieben werden. In solchen Fällen kann das Modell die 
Datenpunkte perfekt anpassen, aber es wird nicht in der Lage sein, neue Datenpunkte vorherzusagen, 
da es das Rauschen in den Daten gelernt hat.

## Aufgabe 3

<!--- ANCHOR: aufgabe_3 --->

Idee: Polynomiale Regression 20. Ordnung mit `minimize`

**(a) Polynomiale Regression 20. Ordnung numerisch implementieren
und an Methylenblau-Daten anwenden.
evtl. mit `polyfit` vergleichen, falls was ähnliches rauskommt.**

**(b) Polynomiale Regression 20. Ordnung mit l1-Norm numerisch implementieren
und an Methylenblau-Daten anwenden.**

**(c) Polynomiale Regression 20. Ordnung mit Ridge- bzw. Lasso-Regression
implementieren und an Methylenblau-Daten anwenden.**

**(d) Erkläre anschaulich, wie Regularisierung wirkt. 
Hinweis: Linreg mit einem Punkt.**
<!--- ANCHOR_END: aufgabe_3 --->

